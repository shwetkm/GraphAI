{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Convolutional Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS3SBY4fOadG",
        "colab_type": "text"
      },
      "source": [
        "![Logo](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5ce6005b22f44fde8ced717c_MD%20Horizontal.png)\n",
        "\n",
        "# Octavian.ai machine learning on graphs course\n",
        "\n",
        "Welcome to our summer course on graph ML.\n",
        "\n",
        "This course is primarily exercise based - you'll learn through reading and writing code, and answering the questions throughout these exercises.\n",
        "\n",
        "[Join our Discord](https://discord.gg/a2Z82Te) to chat with fellow enthusiasts about this exercise and give us feedback to direct the next one.\n",
        "\n",
        "## Exercise 2, graph convolutional networks\n",
        "In this exercise, you will learn how to classify nodes in a graph. We'll do this by creating a graph network that passes messages along the edges of the graph.\n",
        "\n",
        "This technique is very versatile and with creativity can be applied to a wide range of graph problems.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We'll work with the popular [Cora](https://relational.fit.cvut.cz/dataset/CORA) dataset. Using a well known dataset makes this exercise easier as there are lots of existing solutions to look at if you run into trouble. Also, we know a solution is possible, which is not always the case in ML research.\n",
        "\n",
        "Cora is a academic paper citation graph. Its nodes are papers and the edges are citations between them. Each paper also comes with a set of mentioned topics, which we will use to help increase the network's classificiation accuracy.\n",
        "\n",
        "Each node has a classification label, which we will train our network to output:\n",
        "*\t\tCase_Based\n",
        "*\t\tGenetic_Algorithms\n",
        "*\t\tNeural_Networks\n",
        "*\t\tProbabilistic_Methods\n",
        "*\t\tReinforcement_Learning\n",
        "*\t\tRule_Learning\n",
        "*\t\tTheory\n",
        "\n",
        "Some statistics from the Relational Dataset Repository:\n",
        ">The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
        "\n",
        "### Introduction to Graph Convolutional Networks (GCN)\n",
        "\n",
        "A graph convolutional network (GCN) is a machine learning technique for graphs. In a GCN each node has an initial state and directed edge weights:\n",
        "\n",
        "![Illustration of initial graph state](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d4f208e2108a9472bcd5f70_Graph%20illustrations%20(1).png)\n",
        "\n",
        "This initial state could be some known information about a node (in our case, the keywords of the paper), or it could be a fixed/learned/random value.\n",
        "\n",
        "Then a number (determined by the engineer) of GCN layers are executed. In each GCN layer:\n",
        "- Propagate each node's state to its neighbors along the graph's edges (a weighted sum by edge weight)\n",
        "- Then apply the same dense layer *W* to every node, with an optional activation function *σ*\n",
        "\n",
        "![illustration of propagation](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d4f208b8ab9f5fc43094ed8_Graph%20illustrations.png)\n",
        "\n",
        "These two steps can be thought of as the eqivalent of a dense layer in a standard feedforward network, you get to design the architecture by making decisions such as:\n",
        "- How many of these GCN layers to stack\n",
        "- What size should the output node state be after each layer?\n",
        "- What activation function should each GCN layer use?\n",
        "- What regularisation should be applied?\n",
        "\n",
        "The result of this network is a state for each node in the graph. \n",
        "\n",
        "To use these node states as node classifications (as we shall in this exercise), softmax can be used as the final activation function. [This generates a probability-distribution-like vector for each node](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax).\n",
        "\n",
        "### Theoretical background\n",
        "\n",
        "Thomas Kipf has published [really excellent articles](https://tkipf.github.io/graph-convolutional-networks/) about this area of technology, and this tutorial is based off of his basic network structure [outlined here](https://tkipf.github.io/graph-convolutional-networks/).\n",
        "\n",
        "You're encouraged to read Thomas's articles to get the full background on this technique. This exercise focuses on the application of it, as opposed to the background and theory.\n",
        "\n",
        "This area of technology is still in its infancy; The capabilties of GCN have not been fully charted. Whilst working on this exercise, embrace a healthy relish for research and the unknown!\n",
        "\n",
        "### Exercise structure\n",
        "\n",
        "In this exercise you will create a fully functioning graph convolutional network. \n",
        "\n",
        "The exercise is a series of empty functions that you will fill out according to the instructions. There are then a series of unit tests to verify that your code works according to plan.\n",
        "\n",
        "# The exercise\n",
        "\n",
        "## Library setup\n",
        "\n",
        "We'll write our code in Tensorflow 2.0 and Keras. We'll also use Numpy and Scipy for some of the initial data manipulation. Let's load up all the libraries we'll need:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbHCLcYKrVOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfYf55ZqrXAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESFlVsylONbD",
        "colab_type": "code",
        "outputId": "eec6d119-89ac-46ae-f975-3c1fb7ddadc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Set up Python environment\n",
        "# remove older keras and tensorflow versions bundled with colab\n",
        "!pip uninstall -q -y keras\n",
        "!pip uninstall -q -y tensorflow\n",
        "# install tensorflow 2\n",
        "!pip install -q tensorflow==2.0.0-beta1\n",
        "\n",
        "\n",
        "# Import all the libraries we need\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, backend as bk\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import unittest\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping keras as it is not installed.\u001b[0m\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires keras>=2.0.0, which is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0QUSDq5pgUB",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We're going to use the data as prepared in Thomas Kipf's TensorFlow GCN codebase. It's been neatly pickled so that we can easily load it from disk. The following code will download the data, the TensorFlow codebase, and load the data into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUQDQ_HTuoIT",
        "colab_type": "code",
        "outputId": "5f9ec465-bde9-46a9-f2cd-2e9094bab35d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Thanks to Thomas Kipf for sharing this and also for all his work\n",
        "# researching and publicizing GCNs\n",
        "!git clone https://github.com/tkipf/gcn.git\n",
        "\n",
        "# Add the GCN repo to the import path\n",
        "sys.path.append('/content/gcn')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gcn' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKjWq_p1piBj",
        "colab_type": "code",
        "outputId": "c3014bb3-7646-4388-82d2-cd981291d2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Load the data\n",
        "%cd -q /content/gcn/gcn\n",
        "from gcn.utils import load_data\n",
        "\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data('cora')\n",
        "print(\"Loaded\",len(y_train),\"nodes\")\n",
        "print()\n",
        "print(\"-- Data format --\")\n",
        "print(\"Adj:       \", adj.shape,             type(adj), \"number of indices\", len(adj.indices))\n",
        "print(\"y_train:   \", y_train.shape, \"\\t\",   type(y_train))\n",
        "print(\"train_mask:\", train_mask.shape,\"\\t\", type(train_mask))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 2708 nodes\n",
            "\n",
            "-- Data format --\n",
            "Adj:        (2708, 2708) <class 'scipy.sparse.csr.csr_matrix'> number of indices 10556\n",
            "y_train:    (2708, 7) \t <class 'numpy.ndarray'>\n",
            "train_mask: (2708,) \t <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx00AWImwKxl",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "The adjacency matrix is a representation of all the edges in the graph. For each node *i* and *j* in the graph, if they have an edge from *i* to *j* then `adj[i][j] == 1.0`, else `0.0`. Here's an example:\n",
        "\n",
        "![Adjacency matrix format](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d508c94a4a3707d208a52d3_Matrix%20illustration.png)\n",
        "\n",
        "We need to do a little preparation of the adjacency matrix so it'll work well with our GCN layers.\n",
        "\n",
        "First, we need to add self-edges. This allows each node to propagate state back to itself, which allows nodes to retain information.\n",
        "\n",
        "Secondly, the adjacency matrix needs to be [normalised](). This ensures that when node states are propagated during the GCN layer, the size of the result is the average of the neighbors (instead of an ever-increasing sum, depending on the number of incoming edges a node has).\n",
        "\n",
        "The [degree matrix](https://en.wikipedia.org/wiki/Degree_matrix) measures how many edges each node has. We'll essentially divide the adjacency matrix by that so it does not grow the size of the node states each iteration.\n",
        "\n",
        "We'll use the following symmetric normalization technique, which has been noted for its [useful dynamics](https://arxiv.org/abs/1609.02907):\n",
        "\n",
        "<img src=\"https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d508ecc3691c96989e3d4f0_ex2%20sym%20normalize.png\" width=\"400px\"/>\n",
        "\n",
        "Note that the adjacency matrix is [sparse](https://en.wikipedia.org/wiki/Sparse_matrix): instead of storing every value in a *2708 × 2708 × sizeof(float)* memory matrix (hint: that's a lot of memory!), just the non-zero values are stored as a list. Its internal structure is a [list of tuples](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html), each with a numeric value and a coordinate of where in the matrix that value appears.\n",
        "\n",
        "Storing it sparsely greatly reduces the memory footprint of the matrix, making it easier to store, move and process. It does have one downside however: TensorFlow's library function support for sparse matrices is still quite limited.\n",
        "\n",
        "Numpy and Scipy have a richer set of functions for manipulating sparse matrices, so we'll use those to normalize the adjacency matrix prior to feeding it into Tensorflow-Keras.\n",
        "\n",
        "The following functions will be helpful:\n",
        "- `adj.sum(axis)` to get the sum along an axis (hint: the initial adjacency matrix has 1s where there are edges, so summing it along an axis generates the degrees as a vector - which you could diagonalize into a matrix)\n",
        "- `np.power(vector, power)` to square / square-root a matrix\n",
        "- `m[np.isinf(m)] = 0.0` lets you trip infinite values from a matrix, which could occur if you square-root zero\n",
        "- `sp.diags(m)` to get the [diagonal](https://en.wikipedia.org/wiki/Diagonal_matrix) of a matrix\n",
        "- `m.dot(n)` to multiply two matrices together\n",
        "- `sp.eye(m.shape[0])` to get an [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix) the same size as a square matrix `m` - useful for adding self-edges\n",
        "- `m.astype(dtype)` To cast your result to the desired type\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FEg5eu51c3O",
        "colab_type": "code",
        "outputId": "081d498f-c5e9-4257-dbb3-9fbb014bdc71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Takes a scipy csr matrix, returns a csr matrix\n",
        "def sym_normalize_matrix(adj, dtype=np.float32):\n",
        "  '''\n",
        "  Parameters:\n",
        "    adj: The matrix to normalize\n",
        "    dtype: The desired output dtype (e.g. the type of the values in the sparse matrix)\n",
        "  '''\n",
        "  # Apply the matrix normalization D^(-1/2) x A x D^(-1/2) where D is the degree and A the adjacency\n",
        "  \n",
        "  # --- WRITE CODE HERE ---\n",
        "#   print(adj)\n",
        "  D = adj.sum(axis=0)\n",
        "  D = D.tolist()\n",
        "  D = sp.diags(D,[0]).toarray()\n",
        "  D_hat = np.power(D,-0.5)\n",
        "  D_hat[np.isinf(D_hat)] = 0.0\n",
        "#   print(D_hat)\n",
        "  D_hat = sp.csr_matrix(D_hat)\n",
        "  adj_hat = D_hat * adj * D_hat\n",
        "#   adj_hat[np.isinf(adj_hat)] = 0.0\n",
        "  return adj_hat.astype(dtype)\n",
        "\n",
        "\n",
        "# Takes a scipy csr matrix, returns a csr matrix\n",
        "def prepare_adj_matrix(adj):\n",
        "  # Add self-edges to adj, then apply sym_normaliize_matrix to the result\n",
        "  \n",
        "  # --- WRITE CODE HERE ---\n",
        "  i = sp.eye(adj.shape[0])\n",
        "  adj = adj+i\n",
        "  adj = sym_normalize_matrix(adj)\n",
        "  return sp.csr_matrix(adj)\n",
        "\n",
        "\n",
        "prepared_adj = prepare_adj_matrix(adj)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Tests to validate your code\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class TestAdjNormalization(unittest.TestCase):\n",
        "\n",
        "  def assert_csr_close(self, result, expected):\n",
        "    np.testing.assert_allclose(result.indices, expected.indices, err_msg=\"Indices mismatch\")\n",
        "    np.testing.assert_allclose(result.data,  expected.data, err_msg=\"Values mismatch\")\n",
        "    self.assertEqual(result.shape, expected.shape, \"Shape mismatch\")\n",
        "  \n",
        "  def test_prepare(self):\n",
        "    with unittest.mock.patch('__main__.sym_normalize_matrix') as mock_norm:\n",
        "      test_value = sp.csr.csr_matrix(np.array([[0,1],[0,0]], np.float32))\n",
        "      test_value_exp = sp.csr.csr_matrix(np.array([[1,1],[0,1]], np.float32))\n",
        "\n",
        "      result = prepare_adj_matrix(test_value)\n",
        "\n",
        "      assert mock_norm.call_args is not None, \"sym_normalize_matrix should be called\"\n",
        "      args, kwargs = mock_norm.call_args\n",
        "      self.assert_csr_close(args[0], test_value_exp)\n",
        "\n",
        "      print(\"test_prepare success!\")\n",
        "\n",
        "  def test_normalization(self):\n",
        "    \n",
        "    adj = sp.csr_matrix(np.array([\n",
        "        [1.0, 1.0, 0.0],\n",
        "        [1.0, 1.0, 1.0],\n",
        "        [0.0, 0.0, 1.0]\n",
        "    ]))\n",
        "#     print(adj)\n",
        "    result = sym_normalize_matrix(adj, np.float32)\n",
        "    \n",
        "    expected_indices = np.array([[0, 0], [1, 0],      [0, 1],     [1, 1],     [2, 1],     [2, 2]])\n",
        "    expected_values  = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
        "    expected_shape   = (3, 3)\n",
        "\n",
        "    expected = sp.csr_matrix(np.array([\n",
        "      [0.5, 0.5, 0.0],\n",
        "      [0.5, 0.5, 0.5],\n",
        "      [0.0, 0.0, 0.5]\n",
        "    ]))\n",
        "#     print(result.toarray())\n",
        "    self.assert_csr_close(result, expected)\n",
        "    assert result.dtype == np.float32, \"Result of sym_normalize_matrix should have dtype float32, got \" + str(result.dtype)\n",
        "    \n",
        "    print(\"test_normalization success!\")\n",
        "\n",
        "TestAdjNormalization().test_prepare()\n",
        "TestAdjNormalization().test_normalization()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_prepare success!\n",
            "test_normalization success!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in power\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in power\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGUjyl7ZuqRV",
        "colab_type": "text"
      },
      "source": [
        "## Keras layer\n",
        "\n",
        "Now for the exciting part, let's build a GCN layer in Keras that we can use to construct a graph convolutional network.\n",
        "\n",
        "The layer takes the *node_state* as the incoming tensor, then transforms that into a new *node_state*. It performs the following operations (many akin to a standard Dense layer):\n",
        "\n",
        "1.   [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
        "2.   Graph convolution (i.e. multiplying the node state by the weights matrix)\n",
        "3.   Graph propagation (i.e. multiplying the adjacency matrix by the node state)\n",
        "4.   [Activation function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
        "\n",
        "#### Graph convolution\n",
        "This is convolution in the sense that the same parameters are being applied to each node state. This is in the form of a shared matrix, which transforms each node state just as a [dense layer](https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9) would transform the activations in a feed-forward network.\n",
        "\n",
        "#### Implementation details\n",
        "The layer takes the adjacency matrix (the sparse matrix representing graph connectivity) as a parameter in its constructor. The adjacency matrix does not change during training or testing as our graph is static.\n",
        "\n",
        "I've provided the main scaffold for the layer, initialising the weights and constructing the object.\n",
        "\n",
        "#### Your work is to fill out the **call** method of the class *GCNLayer*. I've included comments, and test cases to verify your implementation.\n",
        "\n",
        "Useful functions:\n",
        "- `tf.sparse.sparse_dense_matmul` to multiply a sparse tensor by a dense tensor\n",
        "- `tf.matmul` to multiply a dense tensor by a dense tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9SuJ4FCw0Ca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2b9d3481-da47-42cc-90f1-226ed6834840"
      },
      "source": [
        "class GCNLayer(keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, adjacency, units, activation=tf.identity, dropout=0.0, l2=0.0, dtype=tf.float32, name=None):\n",
        "    '''\n",
        "    Params:\n",
        "      Adjacency: a tf.SparseTensor adjacency matrix\n",
        "      Units: The number of output units per node state\n",
        "      Activation: The activation function to apply to the node states\n",
        "      Dropout: The amount of dropout (0.0 being none, 1.0 being all units) to apply\n",
        "      l2: The amount of L2 regularisation to apply\n",
        "      dtype: The type of values in the tensors this layer will transform\n",
        "      name: The name of this layer \n",
        "    '''\n",
        "    super(GCNLayer, self).__init__(dtype=dtype, name=name)\n",
        "    \n",
        "    self.adjacency = adjacency\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "    self.dropout = dropout\n",
        "    self.l2 = l2\n",
        "    \n",
        "    assert isinstance(adjacency, tf.SparseTensor), \"Adjacency matrix should be a SparseTensor\"\n",
        "    assert adjacency.dtype == self.dtype, \"Adjacency matrix not expected dtype, got \" + str(adjacency.dtype) + \" expected \" + str(self.dtype)\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    '''\n",
        "    This method is called during the initial compilation of our model. Its \n",
        "    primary job is to initialize the weights for this layer.\n",
        "\n",
        "    Params:\n",
        "      input_shape: this is the shape of the input to the layer, in our case an \n",
        "                   array of (NUMBER_NODES, NODE_STATE_SIZE)\n",
        "\n",
        "    We build one weight, w, which will be applied to each node_state. We initialize\n",
        "    it from the uniform distribution, scaled by the size of the matrix. We apply\n",
        "    l2 loss to regularize the matrix\n",
        "    '''\n",
        "    self.w = self.add_weight(\n",
        "      shape=(input_shape[1], self.units),\n",
        "      dtype=self.dtype,\n",
        "      initializer='glorot_uniform',\n",
        "      regularizer=keras.regularizers.l2(self.l2)\n",
        "    )\n",
        "  \n",
        "    \n",
        "  def call(self, node_state):\n",
        "    '''\n",
        "    This method is called to apply the layer to an incoming tensor. This is the\n",
        "    real meat of the model.\n",
        "\n",
        "    Params:\n",
        "      node_state: The tf.Tensor of node states.  Shape (NUMBER_NODES, NODE_STATE_SIZE)\n",
        "\n",
        "    Returns: The transformed node state tf.Tensor\n",
        "    '''\n",
        "    \n",
        "    assert isinstance(node_state, tf.Tensor), \"Layer input should be a Tensor, got \" + str(type(node_state))\n",
        "    assert node_state.dtype == self.dtype, \"Input to layer \" + str(self.name) + \" wrong dtype, got \" + str(node_state.dtype) + \" expected \" + str(self.dtype)\n",
        "    tf.debugging.check_numerics(node_state, \"Input to layer \" + str(self.name) + \" has numerical instability\")\n",
        "\n",
        "    # Apply dropout to the node_state, using the self.dropout as the factor\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = keras.layers.Dropout(self.dropout).apply(node_state)\n",
        "   \n",
        "    # Apply the node convolution: This means to matrix multiply each node_state by our learned parameters `self.w`\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = tf.matmul(node_state,self.w)\n",
        "\n",
        "    # Apply the graph propagation: This means to multiply the\n",
        "    # normalized adjacency matrix by the node state\n",
        "    # You can do this as a single sparse_dense_matmul()\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = tf.sparse.sparse_dense_matmul(self.adjacency,node_state)\n",
        "\n",
        "    # Apply the activation function `self.activation`\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "    node_state = self.activation(node_state)\n",
        "\n",
        "    tf.debugging.check_numerics(node_state, \"Output of layer \" + str(self.name) + \" has numerical instability\")\n",
        "\n",
        "    return node_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Like good engineers, let's validate our code works!\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class TestLayer(unittest.TestCase):\n",
        "\n",
        "  def test_propagate_node_state(self):\n",
        "\n",
        "    sparse_adj = tf.SparseTensor([(0,1), (1,0)], np.array([1.0, 1.0], np.float32), (3, 3))\n",
        "    l = GCNLayer(sparse_adj, units=2, dropout=0.0)\n",
        "    l.build((3, 2,))\n",
        "\n",
        "    weights = tf.constant([\n",
        "      [1.0, 0.0],\n",
        "      [0.0, 1.0]\n",
        "    ])\n",
        "    l.set_weights([weights])\n",
        "\n",
        "    node_state = tf.constant([\n",
        "        [1.0, 0.0],\n",
        "        [0.0, 0.0],\n",
        "        [0.0, 0.0]\n",
        "    ], tf.float32)\n",
        "    \n",
        "    result = l.call(node_state)\n",
        "    \n",
        "    expected_result = [\n",
        "        [0.0, 0.0],\n",
        "        [1.0, 0.0],\n",
        "        [0.0, 0.0]\n",
        "    ]\n",
        "    \n",
        "    np.testing.assert_array_equal(result, expected_result)\n",
        "    \n",
        "    print(\"test_propagate_node_state Success!\")\n",
        "    \n",
        "    \n",
        "  def test_apply_convolution(self):\n",
        "    \n",
        "    node_state = tf.constant([\n",
        "        [ 1.0, 0.0],\n",
        "        [-1.0, 0.0],\n",
        "        [ 0.0, 0.0]\n",
        "    ], tf.float32)\n",
        "\n",
        "    sparse_adj = tf.SparseTensor([(0,0), (1,1), (2,2)], np.array([1.0, 1.0, 1.0], np.float32), (3, 3))\n",
        "    l = GCNLayer(sparse_adj, units=2, dropout=0.0)\n",
        "    l.build((3, 2,))\n",
        "    \n",
        "    weights = tf.constant([\n",
        "        [0.0, 1.0],\n",
        "        [1.0, 0.0]\n",
        "    ])\n",
        "    l.set_weights([weights])\n",
        "    \n",
        "    result = l.call(node_state)\n",
        "    \n",
        "    expected_result = [\n",
        "        [0.0,  1.0],\n",
        "        [0.0, -1.0],\n",
        "        [0.0,  0.0]\n",
        "    ]\n",
        "    \n",
        "    np.testing.assert_array_equal(result, expected_result)\n",
        "    \n",
        "    print(\"test_apply_convolution Success!\")\n",
        "    \n",
        "  def test_layer(self):\n",
        "    \n",
        "    sparse_adj = tf.SparseTensor([(0,1), (1,0)], np.array([1.0, 1.0], np.float32), (3, 3))\n",
        "    \n",
        "    l = GCNLayer(sparse_adj, units=2, activation=keras.backend.relu, dropout=0.0001)\n",
        "    l.build((4, 2,))\n",
        "    \n",
        "    weights = tf.constant([\n",
        "      [0.0, -1.0],\n",
        "      [1.0, 0.0]\n",
        "    ])\n",
        "    l.set_weights([weights])\n",
        "    \n",
        "    node_state = tf.constant([\n",
        "        [-1.0, 0.0],\n",
        "        [ 1.0, 0.0],\n",
        "        [ 0.0, 0.0]\n",
        "    ], tf.float32)\n",
        "    \n",
        "    result = l.call(node_state)\n",
        "    \n",
        "    expected_result = [\n",
        "        [0.0, 0.0],\n",
        "        [0.0, 1.0],\n",
        "        [0.0, 0.0]\n",
        "    ]\n",
        "    \n",
        "    np.testing.assert_allclose(result, expected_result, rtol=1e-03)\n",
        "    \n",
        "    print(\"test_layer Success!\")\n",
        "\n",
        "\n",
        "t = TestLayer()\n",
        "t.test_propagate_node_state()\n",
        "t.test_apply_convolution()\n",
        "t.test_layer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_propagate_node_state Success!\n",
            "test_apply_convolution Success!\n",
            "test_layer Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYRaDxMY-9hH",
        "colab_type": "text"
      },
      "source": [
        "## Keras model\n",
        "\n",
        "Now we have a working GCN layer, let's put it to work in a Keras model. Keras provides a simple interface for doing what we want to do, the Sequential model\n",
        "format. It stacks multiple layers linearly, passing the output of one as the input to the next.\n",
        "\n",
        "We're going to build the following network architecture - two layers, with the following parameters:\n",
        "\n",
        "1.   Output units = 16, activation = relu\n",
        "2.   Output units = number of different labels (7), activation = softmax\n",
        "\n",
        "We'll give each layer our prepared adjacency matrix from earlier.\n",
        "\n",
        "I've provided hyper-parameters for [L2](https://developers.google.com/machine-learning/glossary/#L2_regularization) and [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ragAHoJ3_8Ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L2_FACTOR = 5e-3\n",
        "DROPOUT_FACTOR = 0.5\n",
        "\n",
        "NODE_COUNT   = adj.shape[0]\n",
        "NUM_CLASSES  = 7\n",
        "CLASS_LABELS = list(range(NUM_CLASSES))\n",
        "\n",
        "# Transform our Scipy CSR matrix into a tensorflow SparseTensor\n",
        "coo = prepared_adj.tocoo()\n",
        "indices = np.array(list(zip(coo.row, coo.col)))\n",
        "tf_adj = tf.SparseTensor(indices=indices, values=tf.cast(prepared_adj.data, tf.float32), dense_shape=prepared_adj.shape)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# The model\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  # --- WRITE CODE HERE ---\n",
        "    GCNLayer(tf_adj,16,keras.backend.relu,DROPOUT_FACTOR,L2_FACTOR),\n",
        "    GCNLayer(tf_adj,NUM_CLASSES,keras.backend.softmax)\n",
        "])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TP9cZw3Gjop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecd055ee-8a65-471f-bccd-4d07464d430b"
      },
      "source": [
        "print(NUM_CLASSES)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr2nUPxkAxiB",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Finally, let's train the model. Keras makes this simple for us, with one call to `compile` then `fit`.\n",
        "\n",
        "I've provided the optimizer and loss functions - they're a fairly common setup, using [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) as the optimizer and [categorical cross-entropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/) for the loss calculation.\n",
        "\n",
        "I've provided a helper class to monitor accuracy for us, which I've wired up in the metrics (because of how our data is structured, the normal Keras metrics won't correctly measure accuracy. We need to apply our label mask in both the loss and accuracy calculations, the custom metric below will do this).\n",
        "\n",
        "I've also provided a few graphs - you should try adding more graphs to see the inner workings of the model. Graphing is a valuable research skill."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYw5XEthB9rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metric to measure accuracy of our model, optionally by class.\n",
        "\n",
        "# This is needed as the way we're treating our data doesn't fit the Keras\n",
        "# metrics. In a normal ML flow you have seperate lists of examples (e.g. input \n",
        "# and expected label) for training, testing and validation. You feed one of these\n",
        "# lists in to your training/evaluation loop and measure the model's loss and\n",
        "# accuracy on those examples.\n",
        "\n",
        "# Because our network is a graph, the input to the network is a tensor of node states\n",
        "# and we need to pass the entire set of node states in for the adjacency matrix\n",
        "# to match the shape of the node state matrix. We cannot just pass in the training\n",
        "# nodes and their labels.\n",
        "\n",
        "# Therefore instead we mask the labels output by the network to just the training\n",
        "# or testing set of labels, and measure their accuracy. Keras's sample_weight\n",
        "# mechanism doesn't get applied to accuracy metrics or during testing, therefore\n",
        "# I've implemented our own metric.\n",
        "\n",
        "# This metric has one additional optional feature: It will calculate accuracy for \n",
        "# a single class.\n",
        "\n",
        "# It's important to watch the accuracy by each class label to check the network\n",
        "# is discriminating between them. If a network is struggling to train, one common\n",
        "# failure case is it predicts the same class for all labels as an easy way to \n",
        "# decrease loss. This often presents itself as a train accuracy of 100%/NUM_CLASSES.\n",
        "\n",
        "# By watching individual class acurracies, we can see if the network is learning\n",
        "# to predict each class, or sacrificing some/all classes for one class.\n",
        "\n",
        "class AccuracyByClass(keras.metrics.Metric):\n",
        "  def __init__(self, name, dtype, class_label=None, sample_weight=None, y_true=None):\n",
        "    ''' \n",
        "    Parameters:\n",
        "      name: The name of this metric\n",
        "      dtype: The type of the data being measured\n",
        "      class_label: (Optional) If you supply this, the accuracy will be measured for just that class. Otherwise overall accuracy is measured\n",
        "      sample_weight: The mask you want to apply to the labels\n",
        "      y_true: (Optional) The correct values for the labels output by the network\n",
        "\n",
        "    '''\n",
        "    super().__init__(name, dtype)\n",
        "    \n",
        "    self.class_label = class_label\n",
        "    self.sample_weight = tf.cast(sample_weight, dtype)\n",
        "    self.y_true = y_true\n",
        "\n",
        "    self.correct = tf.Variable(initial_value=0.0, dtype=self._dtype, trainable=False, name='correct_'+str(class_label))\n",
        "    self.total   = tf.Variable(initial_value=0.0, dtype=self._dtype, trainable=False, name='total_'+str(class_label)) \n",
        "    \n",
        "  def reset_states(self):\n",
        "    self.correct.assign(0)\n",
        "    self.total.assign(0)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    # Note that Keras doesn't pass sample_weight into its metrics\n",
        "    # during testing\n",
        "\n",
        "    if self.y_true is not None:\n",
        "      y_true = self.y_true\n",
        "\n",
        "    y_true_classes = tf.argmax(y_true, axis=-1)\n",
        "    y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
        "    correct = tf.equal(y_pred_classes, y_true_classes)\n",
        "\n",
        "    # Create mask\n",
        "    if self.class_label is not None:\n",
        "      mask = tf.cast(tf.equal(y_true_classes, self.class_label), self._dtype)\n",
        "    else:\n",
        "      mask = tf.ones(tf.shape(y_true_classes), self._dtype, 'mask_class_true_ones')\n",
        "    \n",
        "    sample_weight = self.sample_weight\n",
        "\n",
        "    if sample_weight is not None:\n",
        "      mask *= sample_weight\n",
        "\n",
        "    # Apply mask\n",
        "    masked_total_count = tf.reduce_sum(mask)\n",
        "    self.total.assign_add(masked_total_count)\n",
        "\n",
        "    masked_correct = mask * tf.cast(correct, self._dtype)\n",
        "    masked_correct_count = tf.reduce_sum(masked_correct)\n",
        "    self.correct.assign_add(masked_correct_count)\n",
        "  \n",
        "    return self.result()\n",
        "      \n",
        "\n",
        "  def result(self):\n",
        "    return tf.math.divide_no_nan(self.correct, self.total)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imjm5o2XBsHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b273a39b-7fdb-4280-ac92-4ef6df5d0dc2"
      },
      "source": [
        "EPOCH_COUNT   = 200 # Determined by experiment\n",
        "LEARNING_RATE = 0.001 # Learning rate determined through experimentation\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE) \n",
        "\n",
        "metrics = [\n",
        "  AccuracyByClass(\"train_accuracy\", tf.float32, sample_weight=train_mask, y_true=y_train),\n",
        "  AccuracyByClass(\"test_accuracy\",  tf.float32, sample_weight=test_mask,  y_true=y_test),\n",
        "  AccuracyByClass(\"val_accuracy\",   tf.float32, sample_weight=val_mask,   y_true=y_val)\n",
        "]\n",
        "\n",
        "for label in CLASS_LABELS:\n",
        "  metrics.append(AccuracyByClass(\"train_class_acc_\"+str(label), tf.float32, label, sample_weight=train_mask, y_true=y_train))\n",
        "  metrics.append(AccuracyByClass(\"test_class_acc_\"+str(label),  tf.float32, label, sample_weight=test_mask,  y_true=y_test))\n",
        "\n",
        "# Fix the random seeds prior to compiling and training the model - this helps make\n",
        "# results reproduceable\n",
        "np.random.seed(13)\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "# Generate weights, setup optimizer and loss function:\n",
        "model.compile(optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "initial_state = features.todense()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    initial_state, \n",
        "    y_train, \n",
        "    sample_weight=tf.cast(train_mask, tf.float32), # This will be used in loss calculations\n",
        "    validation_data=(initial_state, y_val, val_mask),\n",
        "    epochs=EPOCH_COUNT, \n",
        "    batch_size=NODE_COUNT, # This is unusual in ML - since our adjacency matrix is the whole graph, we want to feed in the whole node_state array in each training step\n",
        "    verbose=1,\n",
        "    shuffle=False # Do not shuffle the order of our input data, since its order matches up to the adjacency matrix\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 2708 samples, validate on 2708 samples\n",
            "Epoch 1/200\n",
            "2708/2708 [==============================] - 1s 222us/sample - loss: 0.2583 - train_accuracy: 0.1500 - test_accuracy: 0.1660 - val_accuracy: 0.1700 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0000e+00 - train_class_acc_1: 0.1000 - test_class_acc_1: 0.2418 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.6944 - train_class_acc_3: 0.1000 - test_class_acc_3: 0.0815 - train_class_acc_4: 0.0000e+00 - test_class_acc_4: 0.1074 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0194 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.5080 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1810 - val_val_accuracy: 0.1860 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.8542 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.0972 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 2/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.2512 - train_accuracy: 0.1714 - test_accuracy: 0.1790 - val_accuracy: 0.1980 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0000e+00 - train_class_acc_1: 0.2000 - test_class_acc_1: 0.1429 - train_class_acc_2: 0.7500 - test_class_acc_2: 0.6250 - train_class_acc_3: 0.1500 - test_class_acc_3: 0.1755 - train_class_acc_4: 0.1000 - test_class_acc_4: 0.1275 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0097 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.5014 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1860 - val_val_accuracy: 0.2060 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0538 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.8000 - val_test_class_acc_2: 0.8542 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1223 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 3/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.2442 - train_accuracy: 0.1857 - test_accuracy: 0.1780 - val_accuracy: 0.2080 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.2500 - test_class_acc_1: 0.1868 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.6458 - train_class_acc_3: 0.0500 - test_class_acc_3: 0.1317 - train_class_acc_4: 0.1000 - test_class_acc_4: 0.1544 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0097 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0156 - val_loss: 0.4943 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1620 - val_val_accuracy: 0.1760 - val_train_class_acc_0: 0.1000 - val_test_class_acc_0: 0.0308 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0659 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.6944 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1317 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 4/200\n",
            "2708/2708 [==============================] - 0s 28us/sample - loss: 0.2372 - train_accuracy: 0.1571 - test_accuracy: 0.1970 - val_accuracy: 0.2060 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0000e+00 - train_class_acc_1: 0.4000 - test_class_acc_1: 0.2857 - train_class_acc_2: 0.6500 - test_class_acc_2: 0.6944 - train_class_acc_3: 0.0500 - test_class_acc_3: 0.1567 - train_class_acc_4: 0.0000e+00 - test_class_acc_4: 0.1342 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0000e+00 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0156 - val_loss: 0.4880 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1570 - val_val_accuracy: 0.1620 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0231 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.6111 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1599 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 5/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.2308 - train_accuracy: 0.2071 - test_accuracy: 0.2230 - val_accuracy: 0.2400 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0000e+00 - train_class_acc_1: 0.3500 - test_class_acc_1: 0.2857 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.7639 - train_class_acc_3: 0.1500 - test_class_acc_3: 0.2038 - train_class_acc_4: 0.1000 - test_class_acc_4: 0.1409 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0097 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4816 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1670 - val_val_accuracy: 0.1520 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0308 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.6181 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1599 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.1000 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 6/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.2243 - train_accuracy: 0.2500 - test_accuracy: 0.2300 - val_accuracy: 0.2120 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.5000 - test_class_acc_1: 0.3626 - train_class_acc_2: 0.9000 - test_class_acc_2: 0.7778 - train_class_acc_3: 0.2500 - test_class_acc_3: 0.2006 - train_class_acc_4: 0.0500 - test_class_acc_4: 0.1275 - train_class_acc_5: 0.0500 - test_class_acc_5: 0.0097 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4756 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1710 - val_val_accuracy: 0.1820 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2418 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.7014 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1003 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0872 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 7/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.2186 - train_accuracy: 0.2357 - test_accuracy: 0.2290 - val_accuracy: 0.2280 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0462 - train_class_acc_1: 0.5500 - test_class_acc_1: 0.4286 - train_class_acc_2: 0.7500 - test_class_acc_2: 0.7083 - train_class_acc_3: 0.2500 - test_class_acc_3: 0.2194 - train_class_acc_4: 0.1000 - test_class_acc_4: 0.0738 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0097 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4696 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1660 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0385 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.6667 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1411 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 8/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.2125 - train_accuracy: 0.2429 - test_accuracy: 0.2380 - val_accuracy: 0.2600 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.4500 - test_class_acc_1: 0.3846 - train_class_acc_2: 0.8000 - test_class_acc_2: 0.7361 - train_class_acc_3: 0.3500 - test_class_acc_3: 0.2288 - train_class_acc_4: 0.1000 - test_class_acc_4: 0.1141 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0485 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4638 - val_train_accuracy: 0.2000 - val_test_accuracy: 0.1850 - val_val_accuracy: 0.1960 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0846 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.9000 - val_test_class_acc_2: 0.6319 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1787 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1007 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 9/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.2067 - train_accuracy: 0.2500 - test_accuracy: 0.2390 - val_accuracy: 0.2500 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0385 - train_class_acc_1: 0.3000 - test_class_acc_1: 0.4396 - train_class_acc_2: 0.9000 - test_class_acc_2: 0.7153 - train_class_acc_3: 0.2500 - test_class_acc_3: 0.2288 - train_class_acc_4: 0.1500 - test_class_acc_4: 0.0940 - train_class_acc_5: 0.1000 - test_class_acc_5: 0.0291 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0156 - val_loss: 0.4586 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.1700 - val_val_accuracy: 0.1460 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0462 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.5556 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2006 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0671 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 10/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.2011 - train_accuracy: 0.2714 - test_accuracy: 0.2700 - val_accuracy: 0.2640 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.5000 - test_class_acc_1: 0.4396 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.7917 - train_class_acc_3: 0.3000 - test_class_acc_3: 0.2790 - train_class_acc_4: 0.1000 - test_class_acc_4: 0.1409 - train_class_acc_5: 0.0500 - test_class_acc_5: 0.0291 - train_class_acc_6: 0.0500 - test_class_acc_6: 0.0156 - val_loss: 0.4537 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1590 - val_val_accuracy: 0.1440 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0538 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.4861 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.2006 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 11/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1960 - train_accuracy: 0.2857 - test_accuracy: 0.2550 - val_accuracy: 0.2440 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.7500 - test_class_acc_1: 0.5714 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.7083 - train_class_acc_3: 0.2500 - test_class_acc_3: 0.2665 - train_class_acc_4: 0.1000 - test_class_acc_4: 0.0805 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0194 - train_class_acc_6: 0.0500 - test_class_acc_6: 0.0156 - val_loss: 0.4484 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1900 - val_val_accuracy: 0.1660 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0692 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.2884 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 12/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1908 - train_accuracy: 0.3214 - test_accuracy: 0.2950 - val_accuracy: 0.2880 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0385 - train_class_acc_1: 0.8500 - test_class_acc_1: 0.6703 - train_class_acc_2: 0.8000 - test_class_acc_2: 0.7153 - train_class_acc_3: 0.3000 - test_class_acc_3: 0.2978 - train_class_acc_4: 0.2000 - test_class_acc_4: 0.1812 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0291 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0156 - val_loss: 0.4434 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1500 - val_val_accuracy: 0.1580 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0231 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.5069 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1755 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0738 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 13/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1860 - train_accuracy: 0.3143 - test_accuracy: 0.3170 - val_accuracy: 0.3120 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0385 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.6044 - train_class_acc_2: 0.7500 - test_class_acc_2: 0.7431 - train_class_acc_3: 0.3000 - test_class_acc_3: 0.3511 - train_class_acc_4: 0.1500 - test_class_acc_4: 0.2013 - train_class_acc_5: 0.0500 - test_class_acc_5: 0.0777 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4388 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1690 - val_val_accuracy: 0.1560 - val_train_class_acc_0: 0.1000 - val_test_class_acc_0: 0.0538 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.4722 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.2508 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 14/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.1812 - train_accuracy: 0.3500 - test_accuracy: 0.3160 - val_accuracy: 0.3140 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0000e+00 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.6374 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.7361 - train_class_acc_3: 0.5000 - test_class_acc_3: 0.3824 - train_class_acc_4: 0.1500 - test_class_acc_4: 0.1879 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0194 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4345 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1950 - val_val_accuracy: 0.1920 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0538 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.4028 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.3009 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 15/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1769 - train_accuracy: 0.3286 - test_accuracy: 0.3430 - val_accuracy: 0.3220 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0692 - train_class_acc_1: 0.8000 - test_class_acc_1: 0.6374 - train_class_acc_2: 0.8000 - test_class_acc_2: 0.7569 - train_class_acc_3: 0.4000 - test_class_acc_3: 0.4232 - train_class_acc_4: 0.1500 - test_class_acc_4: 0.1879 - train_class_acc_5: 0.1500 - test_class_acc_5: 0.0388 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4301 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.1730 - val_val_accuracy: 0.1580 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0538 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.3611 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.2790 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 16/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1724 - train_accuracy: 0.3357 - test_accuracy: 0.3380 - val_accuracy: 0.3220 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0231 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.7363 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.7500 - train_class_acc_3: 0.3500 - test_class_acc_3: 0.3887 - train_class_acc_4: 0.2500 - test_class_acc_4: 0.2148 - train_class_acc_5: 0.0000e+00 - test_class_acc_5: 0.0388 - train_class_acc_6: 0.0000e+00 - test_class_acc_6: 0.0000e+00 - val_loss: 0.4262 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1950 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0769 - val_train_class_acc_1: 0.3500 - val_test_class_acc_1: 0.4066 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.3542 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.2665 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 17/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1684 - train_accuracy: 0.3429 - test_accuracy: 0.3560 - val_accuracy: 0.3720 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0692 - train_class_acc_1: 0.7500 - test_class_acc_1: 0.7143 - train_class_acc_2: 0.7500 - test_class_acc_2: 0.7569 - train_class_acc_3: 0.4500 - test_class_acc_3: 0.4107 - train_class_acc_4: 0.1500 - test_class_acc_4: 0.2215 - train_class_acc_5: 0.1500 - test_class_acc_5: 0.0680 - train_class_acc_6: 0.0500 - test_class_acc_6: 0.0312 - val_loss: 0.4220 - val_train_accuracy: 0.1857 - val_test_accuracy: 0.1930 - val_val_accuracy: 0.2120 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.3000 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.3750 - val_train_class_acc_3: 0.5000 - val_test_class_acc_3: 0.3668 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 18/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.1644 - train_accuracy: 0.3929 - test_accuracy: 0.3540 - val_accuracy: 0.3640 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.8000 - test_class_acc_1: 0.6813 - train_class_acc_2: 0.9000 - test_class_acc_2: 0.7569 - train_class_acc_3: 0.5000 - test_class_acc_3: 0.4357 - train_class_acc_4: 0.2000 - test_class_acc_4: 0.2215 - train_class_acc_5: 0.0500 - test_class_acc_5: 0.0388 - train_class_acc_6: 0.3000 - test_class_acc_6: 0.0469 - val_loss: 0.4186 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.2040 - val_val_accuracy: 0.2040 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.3472 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.4107 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 19/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1606 - train_accuracy: 0.4214 - test_accuracy: 0.3920 - val_accuracy: 0.3720 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.1154 - train_class_acc_1: 0.8500 - test_class_acc_1: 0.7363 - train_class_acc_2: 0.9000 - test_class_acc_2: 0.7708 - train_class_acc_3: 0.5500 - test_class_acc_3: 0.4859 - train_class_acc_4: 0.2500 - test_class_acc_4: 0.2416 - train_class_acc_5: 0.0500 - test_class_acc_5: 0.0485 - train_class_acc_6: 0.2000 - test_class_acc_6: 0.0469 - val_loss: 0.4146 - val_train_accuracy: 0.1000 - val_test_accuracy: 0.1790 - val_val_accuracy: 0.1820 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0308 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.2569 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3824 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 20/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1570 - train_accuracy: 0.3786 - test_accuracy: 0.3950 - val_accuracy: 0.3740 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.8500 - test_class_acc_1: 0.7912 - train_class_acc_2: 0.8000 - test_class_acc_2: 0.7847 - train_class_acc_3: 0.5500 - test_class_acc_3: 0.5110 - train_class_acc_4: 0.2000 - test_class_acc_4: 0.2416 - train_class_acc_5: 0.1000 - test_class_acc_5: 0.0485 - train_class_acc_6: 0.1500 - test_class_acc_6: 0.0312 - val_loss: 0.4110 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1680 - val_val_accuracy: 0.2000 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0462 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.1944 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.3668 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0201 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 21/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1534 - train_accuracy: 0.4786 - test_accuracy: 0.4250 - val_accuracy: 0.4120 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.8681 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.7708 - train_class_acc_3: 0.5500 - test_class_acc_3: 0.5768 - train_class_acc_4: 0.3500 - test_class_acc_4: 0.2349 - train_class_acc_5: 0.2000 - test_class_acc_5: 0.0777 - train_class_acc_6: 0.3000 - test_class_acc_6: 0.0625 - val_loss: 0.4078 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.1950 - val_val_accuracy: 0.2020 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.1500 - val_test_class_acc_2: 0.2708 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.4138 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 22/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1504 - train_accuracy: 0.4214 - test_accuracy: 0.4260 - val_accuracy: 0.4140 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0231 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.7802 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.7917 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5956 - train_class_acc_4: 0.1500 - test_class_acc_4: 0.2617 - train_class_acc_5: 0.1500 - test_class_acc_5: 0.0485 - train_class_acc_6: 0.0500 - test_class_acc_6: 0.0625 - val_loss: 0.4050 - val_train_accuracy: 0.1071 - val_test_accuracy: 0.1940 - val_val_accuracy: 0.1760 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.1000 - val_test_class_acc_2: 0.1806 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.4639 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 23/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.1469 - train_accuracy: 0.5143 - test_accuracy: 0.4480 - val_accuracy: 0.4240 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8352 - train_class_acc_2: 0.9000 - test_class_acc_2: 0.7639 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.6332 - train_class_acc_4: 0.3500 - test_class_acc_4: 0.2752 - train_class_acc_5: 0.3000 - test_class_acc_5: 0.0777 - train_class_acc_6: 0.3000 - test_class_acc_6: 0.0625 - val_loss: 0.4018 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.1700 - val_val_accuracy: 0.1920 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.0500 - val_test_class_acc_2: 0.2014 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.3918 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 24/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1443 - train_accuracy: 0.4786 - test_accuracy: 0.4700 - val_accuracy: 0.4740 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0846 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8132 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8472 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.6332 - train_class_acc_4: 0.3500 - test_class_acc_4: 0.3087 - train_class_acc_5: 0.0500 - test_class_acc_5: 0.1165 - train_class_acc_6: 0.1500 - test_class_acc_6: 0.0469 - val_loss: 0.3985 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1870 - val_val_accuracy: 0.1880 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.1736 - val_train_class_acc_3: 0.5500 - val_test_class_acc_3: 0.4608 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 25/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1410 - train_accuracy: 0.4929 - test_accuracy: 0.4570 - val_accuracy: 0.4220 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.7912 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.7847 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.6677 - train_class_acc_4: 0.3500 - test_class_acc_4: 0.2685 - train_class_acc_5: 0.0500 - test_class_acc_5: 0.0777 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.0625 - val_loss: 0.3958 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2240 - val_val_accuracy: 0.2400 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.1042 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.5799 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 26/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1384 - train_accuracy: 0.5357 - test_accuracy: 0.4770 - val_accuracy: 0.4800 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8462 - train_class_acc_2: 0.9000 - test_class_acc_2: 0.8472 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.6395 - train_class_acc_4: 0.3500 - test_class_acc_4: 0.3758 - train_class_acc_5: 0.3000 - test_class_acc_5: 0.0777 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.0938 - val_loss: 0.3933 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.2190 - val_val_accuracy: 0.2460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.2000 - val_test_class_acc_2: 0.1667 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.5110 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0485 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 27/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.1356 - train_accuracy: 0.5929 - test_accuracy: 0.4590 - val_accuracy: 0.4440 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0231 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.7912 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.7917 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6332 - train_class_acc_4: 0.4500 - test_class_acc_4: 0.3020 - train_class_acc_5: 0.4500 - test_class_acc_5: 0.1456 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.1250 - val_loss: 0.3906 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.2420 - val_val_accuracy: 0.2360 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.2292 - val_train_class_acc_3: 0.6500 - val_test_class_acc_3: 0.5893 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 28/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1331 - train_accuracy: 0.5357 - test_accuracy: 0.4870 - val_accuracy: 0.4900 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8333 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.6270 - train_class_acc_4: 0.4500 - test_class_acc_4: 0.3960 - train_class_acc_5: 0.2000 - test_class_acc_5: 0.1456 - train_class_acc_6: 0.3500 - test_class_acc_6: 0.1250 - val_loss: 0.3883 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.2090 - val_val_accuracy: 0.2220 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.2747 - val_train_class_acc_2: 0.2000 - val_test_class_acc_2: 0.2083 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.4671 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 29/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.1309 - train_accuracy: 0.5500 - test_accuracy: 0.4870 - val_accuracy: 0.5020 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8352 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.7847 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.6364 - train_class_acc_4: 0.5500 - test_class_acc_4: 0.4966 - train_class_acc_5: 0.2000 - test_class_acc_5: 0.1068 - train_class_acc_6: 0.3500 - test_class_acc_6: 0.0938 - val_loss: 0.3861 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.2180 - val_val_accuracy: 0.2340 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.1500 - val_test_class_acc_2: 0.2222 - val_train_class_acc_3: 0.7000 - val_test_class_acc_3: 0.5298 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 30/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1285 - train_accuracy: 0.5429 - test_accuracy: 0.4840 - val_accuracy: 0.4900 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8462 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.8194 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6771 - train_class_acc_4: 0.4500 - test_class_acc_4: 0.3490 - train_class_acc_5: 0.2000 - test_class_acc_5: 0.1359 - train_class_acc_6: 0.3000 - test_class_acc_6: 0.0938 - val_loss: 0.3839 - val_train_accuracy: 0.1929 - val_test_accuracy: 0.2040 - val_val_accuracy: 0.1900 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.1667 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.5016 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 31/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.1266 - train_accuracy: 0.5500 - test_accuracy: 0.4910 - val_accuracy: 0.4880 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.8681 - train_class_acc_2: 0.8500 - test_class_acc_2: 0.8472 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.6552 - train_class_acc_4: 0.4500 - test_class_acc_4: 0.3289 - train_class_acc_5: 0.3500 - test_class_acc_5: 0.2427 - train_class_acc_6: 0.4500 - test_class_acc_6: 0.0938 - val_loss: 0.3816 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.2070 - val_val_accuracy: 0.2120 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.2000 - val_test_class_acc_2: 0.2222 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.4608 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0738 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 32/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.1242 - train_accuracy: 0.5571 - test_accuracy: 0.4850 - val_accuracy: 0.4820 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8542 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.6426 - train_class_acc_4: 0.5000 - test_class_acc_4: 0.3624 - train_class_acc_5: 0.3000 - test_class_acc_5: 0.0971 - train_class_acc_6: 0.3500 - test_class_acc_6: 0.1094 - val_loss: 0.3797 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.2310 - val_val_accuracy: 0.2200 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.2778 - val_train_class_acc_3: 0.5000 - val_test_class_acc_3: 0.5580 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0201 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 33/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.1224 - train_accuracy: 0.5500 - test_accuracy: 0.5230 - val_accuracy: 0.5020 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8542 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.7116 - train_class_acc_4: 0.4500 - test_class_acc_4: 0.4228 - train_class_acc_5: 0.2000 - test_class_acc_5: 0.1845 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.1250 - val_loss: 0.3775 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2270 - val_val_accuracy: 0.2540 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.1000 - val_test_class_acc_2: 0.1667 - val_train_class_acc_3: 0.6500 - val_test_class_acc_3: 0.5549 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 34/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.1206 - train_accuracy: 0.6000 - test_accuracy: 0.5090 - val_accuracy: 0.5240 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 0.9000 - test_class_acc_2: 0.8889 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6677 - train_class_acc_4: 0.5500 - test_class_acc_4: 0.4094 - train_class_acc_5: 0.4500 - test_class_acc_5: 0.1359 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.0938 - val_loss: 0.3758 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2000 - val_val_accuracy: 0.2140 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.1597 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.5235 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0134 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 35/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1187 - train_accuracy: 0.6071 - test_accuracy: 0.5390 - val_accuracy: 0.5180 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8750 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.7085 - train_class_acc_4: 0.6500 - test_class_acc_4: 0.4899 - train_class_acc_5: 0.5000 - test_class_acc_5: 0.2233 - train_class_acc_6: 0.3500 - test_class_acc_6: 0.1094 - val_loss: 0.3740 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2040 - val_val_accuracy: 0.1960 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.1500 - val_test_class_acc_2: 0.1736 - val_train_class_acc_3: 0.6500 - val_test_class_acc_3: 0.5110 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 36/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.1168 - train_accuracy: 0.6143 - test_accuracy: 0.5010 - val_accuracy: 0.5140 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8125 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6458 - train_class_acc_4: 0.6500 - test_class_acc_4: 0.4832 - train_class_acc_5: 0.3500 - test_class_acc_5: 0.1553 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.1094 - val_loss: 0.3730 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2360 - val_val_accuracy: 0.2420 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.2000 - val_test_class_acc_2: 0.2014 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.5643 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 37/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.1154 - train_accuracy: 0.6214 - test_accuracy: 0.5310 - val_accuracy: 0.5400 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.8472 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6959 - train_class_acc_4: 0.7000 - test_class_acc_4: 0.5034 - train_class_acc_5: 0.5000 - test_class_acc_5: 0.1942 - train_class_acc_6: 0.3500 - test_class_acc_6: 0.1094 - val_loss: 0.3713 - val_train_accuracy: 0.1929 - val_test_accuracy: 0.2190 - val_val_accuracy: 0.1960 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.2361 - val_train_class_acc_3: 0.7000 - val_test_class_acc_3: 0.5078 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 38/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1137 - train_accuracy: 0.6500 - test_accuracy: 0.5600 - val_accuracy: 0.5560 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0769 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.8750 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.7335 - train_class_acc_4: 0.6500 - test_class_acc_4: 0.5302 - train_class_acc_5: 0.5000 - test_class_acc_5: 0.2136 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.0938 - val_loss: 0.3698 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.2090 - val_val_accuracy: 0.2360 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.2639 - val_train_class_acc_3: 0.5500 - val_test_class_acc_3: 0.4984 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0067 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 39/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1127 - train_accuracy: 0.6214 - test_accuracy: 0.5240 - val_accuracy: 0.5060 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9028 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6834 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.4631 - train_class_acc_5: 0.4000 - test_class_acc_5: 0.1456 - train_class_acc_6: 0.3000 - test_class_acc_6: 0.0781 - val_loss: 0.3685 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.1870 - val_val_accuracy: 0.1980 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.2153 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.4483 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 40/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1115 - train_accuracy: 0.6429 - test_accuracy: 0.5650 - val_accuracy: 0.5540 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.7241 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.5906 - train_class_acc_5: 0.5500 - test_class_acc_5: 0.2427 - train_class_acc_6: 0.3500 - test_class_acc_6: 0.0781 - val_loss: 0.3668 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.2390 - val_val_accuracy: 0.2300 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.3542 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.5423 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 41/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1101 - train_accuracy: 0.6357 - test_accuracy: 0.5570 - val_accuracy: 0.5320 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8889 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.7429 - train_class_acc_4: 0.7000 - test_class_acc_4: 0.5503 - train_class_acc_5: 0.6000 - test_class_acc_5: 0.2427 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.0625 - val_loss: 0.3658 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.2340 - val_val_accuracy: 0.2200 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.2418 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.2917 - val_train_class_acc_3: 0.6500 - val_test_class_acc_3: 0.5235 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0134 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 42/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1086 - train_accuracy: 0.6214 - test_accuracy: 0.5550 - val_accuracy: 0.5360 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0000e+00 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8819 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.7179 - train_class_acc_4: 0.7000 - test_class_acc_4: 0.6107 - train_class_acc_5: 0.5000 - test_class_acc_5: 0.2330 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.0781 - val_loss: 0.3644 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.2170 - val_val_accuracy: 0.2020 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.2708 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.4890 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 43/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1072 - train_accuracy: 0.6714 - test_accuracy: 0.5580 - val_accuracy: 0.5680 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0154 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.7116 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.6174 - train_class_acc_5: 0.5000 - test_class_acc_5: 0.1845 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.0938 - val_loss: 0.3635 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.2180 - val_val_accuracy: 0.1900 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.1500 - val_test_class_acc_2: 0.2222 - val_train_class_acc_3: 0.7500 - val_test_class_acc_3: 0.5361 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 44/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1066 - train_accuracy: 0.6286 - test_accuracy: 0.5650 - val_accuracy: 0.5580 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9028 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.7147 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.6577 - train_class_acc_5: 0.5000 - test_class_acc_5: 0.1942 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.1250 - val_loss: 0.3626 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.2180 - val_val_accuracy: 0.1960 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.4167 - val_train_class_acc_3: 0.5500 - val_test_class_acc_3: 0.4263 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 45/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1054 - train_accuracy: 0.6786 - test_accuracy: 0.5680 - val_accuracy: 0.5680 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8819 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.7179 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.6309 - train_class_acc_5: 0.5000 - test_class_acc_5: 0.2136 - train_class_acc_6: 0.4500 - test_class_acc_6: 0.1250 - val_loss: 0.3622 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.2150 - val_val_accuracy: 0.2460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.3000 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.2000 - val_test_class_acc_2: 0.3403 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.4451 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 46/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.1044 - train_accuracy: 0.6714 - test_accuracy: 0.5930 - val_accuracy: 0.5900 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0000e+00 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.7524 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.6644 - train_class_acc_5: 0.5500 - test_class_acc_5: 0.3301 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.1719 - val_loss: 0.3608 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.2020 - val_val_accuracy: 0.2160 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.3681 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.4138 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 47/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1035 - train_accuracy: 0.6786 - test_accuracy: 0.5800 - val_accuracy: 0.5600 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9167 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.7147 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.6376 - train_class_acc_5: 0.6000 - test_class_acc_5: 0.2913 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.1562 - val_loss: 0.3600 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2120 - val_val_accuracy: 0.2220 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.3889 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.4326 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0201 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 48/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1028 - train_accuracy: 0.6571 - test_accuracy: 0.5620 - val_accuracy: 0.5680 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6771 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.6577 - train_class_acc_5: 0.6000 - test_class_acc_5: 0.2524 - train_class_acc_6: 0.4000 - test_class_acc_6: 0.1094 - val_loss: 0.3593 - val_train_accuracy: 0.1929 - val_test_accuracy: 0.2420 - val_val_accuracy: 0.2060 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.4861 - val_train_class_acc_3: 0.6000 - val_test_class_acc_3: 0.4828 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 49/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.1017 - train_accuracy: 0.7000 - test_accuracy: 0.6050 - val_accuracy: 0.5920 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.7398 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7315 - train_class_acc_5: 0.6000 - test_class_acc_5: 0.2913 - train_class_acc_6: 0.4500 - test_class_acc_6: 0.1406 - val_loss: 0.3580 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.2200 - val_val_accuracy: 0.2620 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.5000 - val_test_class_acc_3: 0.4326 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0671 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 50/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.1013 - train_accuracy: 0.7000 - test_accuracy: 0.5670 - val_accuracy: 0.5660 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8889 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6991 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.6174 - train_class_acc_5: 0.5500 - test_class_acc_5: 0.2427 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2656 - val_loss: 0.3580 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1920 - val_val_accuracy: 0.1840 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.3681 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.3793 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 51/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.1003 - train_accuracy: 0.7000 - test_accuracy: 0.5750 - val_accuracy: 0.5740 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8958 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.7273 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.6309 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.2621 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.1719 - val_loss: 0.3569 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.2210 - val_val_accuracy: 0.2300 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4583 - val_train_class_acc_3: 0.6500 - val_test_class_acc_3: 0.4295 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 52/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0997 - train_accuracy: 0.6929 - test_accuracy: 0.5840 - val_accuracy: 0.5840 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6991 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.6510 - train_class_acc_5: 0.6000 - test_class_acc_5: 0.4175 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.1406 - val_loss: 0.3561 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.2010 - val_val_accuracy: 0.2260 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.4167 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.3950 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 53/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0991 - train_accuracy: 0.7071 - test_accuracy: 0.6020 - val_accuracy: 0.5980 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9028 - train_class_acc_3: 0.9500 - test_class_acc_3: 0.7429 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.7315 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.3398 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.1250 - val_loss: 0.3557 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2060 - val_val_accuracy: 0.2200 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5069 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3856 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0201 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 54/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0983 - train_accuracy: 0.7000 - test_accuracy: 0.6110 - val_accuracy: 0.5820 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6959 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.7315 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4369 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.2500 - val_loss: 0.3547 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.2220 - val_val_accuracy: 0.2000 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5139 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.4044 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 55/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0981 - train_accuracy: 0.6786 - test_accuracy: 0.5940 - val_accuracy: 0.5720 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.7147 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.6913 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.3010 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.1094 - val_loss: 0.3546 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1960 - val_val_accuracy: 0.2200 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.3403 - val_train_class_acc_3: 0.5000 - val_test_class_acc_3: 0.3981 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 56/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0974 - train_accuracy: 0.7000 - test_accuracy: 0.6120 - val_accuracy: 0.5860 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.7053 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7651 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4369 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.1250 - val_loss: 0.3547 - val_train_accuracy: 0.1857 - val_test_accuracy: 0.2110 - val_val_accuracy: 0.2280 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.4653 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.3950 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 57/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0970 - train_accuracy: 0.7214 - test_accuracy: 0.5920 - val_accuracy: 0.5860 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0154 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6489 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.7517 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.4078 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2031 - val_loss: 0.3541 - val_train_accuracy: 0.1929 - val_test_accuracy: 0.2160 - val_val_accuracy: 0.2020 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4653 - val_train_class_acc_3: 0.5500 - val_test_class_acc_3: 0.4295 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0201 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 58/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0959 - train_accuracy: 0.7000 - test_accuracy: 0.6040 - val_accuracy: 0.5820 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6991 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7315 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4369 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.1562 - val_loss: 0.3536 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.2220 - val_val_accuracy: 0.2120 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5417 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.3668 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 59/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0961 - train_accuracy: 0.7286 - test_accuracy: 0.6010 - val_accuracy: 0.5700 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.9500 - test_class_acc_3: 0.6708 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7718 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4660 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.1406 - val_loss: 0.3536 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.2120 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.4375 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.4044 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0156\n",
            "Epoch 60/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0953 - train_accuracy: 0.7214 - test_accuracy: 0.6200 - val_accuracy: 0.5960 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6959 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7584 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.4757 - train_class_acc_6: 0.4500 - test_class_acc_6: 0.1875 - val_loss: 0.3530 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.2120 - val_val_accuracy: 0.2040 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.3500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4792 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.4044 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 61/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0950 - train_accuracy: 0.7357 - test_accuracy: 0.6130 - val_accuracy: 0.5980 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.7335 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.6913 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4175 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2031 - val_loss: 0.3535 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.2310 - val_val_accuracy: 0.2320 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.4653 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.4639 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 62/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0946 - train_accuracy: 0.6857 - test_accuracy: 0.6180 - val_accuracy: 0.6000 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.7053 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.7181 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4854 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.1875 - val_loss: 0.3515 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1980 - val_val_accuracy: 0.1940 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.4444 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3605 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0671 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 63/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0944 - train_accuracy: 0.7143 - test_accuracy: 0.5970 - val_accuracy: 0.5920 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6928 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.7248 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4175 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.1719 - val_loss: 0.3521 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.2130 - val_val_accuracy: 0.1960 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2198 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.3762 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 64/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0947 - train_accuracy: 0.7143 - test_accuracy: 0.6190 - val_accuracy: 0.5820 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0615 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6991 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7450 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4078 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.2188 - val_loss: 0.3518 - val_train_accuracy: 0.0786 - val_test_accuracy: 0.1710 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.2500 - val_test_class_acc_2: 0.4167 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3103 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 65/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0935 - train_accuracy: 0.7071 - test_accuracy: 0.6090 - val_accuracy: 0.6000 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.7116 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.7584 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.3301 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2344 - val_loss: 0.3524 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1720 - val_val_accuracy: 0.2220 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0549 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.2947 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 66/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0936 - train_accuracy: 0.7429 - test_accuracy: 0.6060 - val_accuracy: 0.5980 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.9500 - test_class_acc_3: 0.6865 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.7517 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4563 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2031 - val_loss: 0.3514 - val_train_accuracy: 0.1071 - val_test_accuracy: 0.1870 - val_val_accuracy: 0.1880 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0549 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.4167 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.3574 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 67/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0932 - train_accuracy: 0.7214 - test_accuracy: 0.6190 - val_accuracy: 0.6100 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6865 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7651 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4369 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2812 - val_loss: 0.3511 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.2120 - val_val_accuracy: 0.2420 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5069 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3824 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 68/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0931 - train_accuracy: 0.7143 - test_accuracy: 0.5970 - val_accuracy: 0.5940 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6301 - train_class_acc_4: 0.8000 - test_class_acc_4: 0.7584 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4369 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2812 - val_loss: 0.3502 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.2150 - val_val_accuracy: 0.2020 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5972 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3542 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 69/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0924 - train_accuracy: 0.7286 - test_accuracy: 0.6050 - val_accuracy: 0.5960 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6646 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7852 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.3786 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.2500 - val_loss: 0.3513 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1900 - val_val_accuracy: 0.1900 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4028 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.3480 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 70/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0924 - train_accuracy: 0.7286 - test_accuracy: 0.6020 - val_accuracy: 0.5880 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0231 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6740 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7517 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4175 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2344 - val_loss: 0.3501 - val_train_accuracy: 0.1857 - val_test_accuracy: 0.2280 - val_val_accuracy: 0.2000 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5069 - val_train_class_acc_3: 0.6500 - val_test_class_acc_3: 0.4357 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0268 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 71/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0917 - train_accuracy: 0.7429 - test_accuracy: 0.6020 - val_accuracy: 0.5960 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6426 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7450 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4078 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3438 - val_loss: 0.3499 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.1930 - val_val_accuracy: 0.1920 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4375 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.3354 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 72/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0914 - train_accuracy: 0.7357 - test_accuracy: 0.6200 - val_accuracy: 0.6020 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0154 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6771 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7785 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5146 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2344 - val_loss: 0.3499 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1970 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4583 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3480 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 73/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0914 - train_accuracy: 0.7286 - test_accuracy: 0.6080 - val_accuracy: 0.5860 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6426 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8054 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4951 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2188 - val_loss: 0.3506 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.2110 - val_val_accuracy: 0.2060 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4792 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.3699 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 74/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0916 - train_accuracy: 0.7429 - test_accuracy: 0.6080 - val_accuracy: 0.6140 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0385 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6364 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.7987 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4369 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2656 - val_loss: 0.3494 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1700 - val_val_accuracy: 0.1620 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2947 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0067 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 75/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0914 - train_accuracy: 0.7214 - test_accuracy: 0.6210 - val_accuracy: 0.6160 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6897 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7718 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.4854 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2812 - val_loss: 0.3497 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1850 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4583 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.3166 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 76/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0906 - train_accuracy: 0.7571 - test_accuracy: 0.6250 - val_accuracy: 0.6300 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.9500 - test_class_acc_3: 0.6740 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.8188 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4466 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2344 - val_loss: 0.3504 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.1780 - val_val_accuracy: 0.1640 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4583 - val_train_class_acc_3: 0.5000 - val_test_class_acc_3: 0.2759 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 77/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0911 - train_accuracy: 0.7286 - test_accuracy: 0.5950 - val_accuracy: 0.6200 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8462 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.6176 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7651 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4757 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2344 - val_loss: 0.3488 - val_train_accuracy: 0.1071 - val_test_accuracy: 0.1680 - val_val_accuracy: 0.2080 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.4097 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.2759 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 78/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0902 - train_accuracy: 0.7143 - test_accuracy: 0.6190 - val_accuracy: 0.6020 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0692 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6395 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7987 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4951 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2656 - val_loss: 0.3493 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.2110 - val_val_accuracy: 0.2140 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4722 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.3574 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1007 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 79/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0906 - train_accuracy: 0.7786 - test_accuracy: 0.6140 - val_accuracy: 0.6360 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6646 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.7785 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.4757 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2031 - val_loss: 0.3488 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1770 - val_val_accuracy: 0.1660 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4097 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.2727 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 80/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0900 - train_accuracy: 0.7357 - test_accuracy: 0.6240 - val_accuracy: 0.6140 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0923 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6583 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7785 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5146 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2656 - val_loss: 0.3485 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.1870 - val_val_accuracy: 0.1880 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5000 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.2884 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0738 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 81/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0899 - train_accuracy: 0.7000 - test_accuracy: 0.6210 - val_accuracy: 0.6000 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.6708 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4757 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.2656 - val_loss: 0.3489 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1610 - val_val_accuracy: 0.1460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.4444 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.2288 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 82/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0899 - train_accuracy: 0.7643 - test_accuracy: 0.6330 - val_accuracy: 0.6020 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0692 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6865 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8121 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5146 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2188 - val_loss: 0.3477 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.2170 - val_val_accuracy: 0.2060 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.3000 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.5694 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3448 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0537 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 83/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0897 - train_accuracy: 0.7429 - test_accuracy: 0.6360 - val_accuracy: 0.6180 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.9500 - test_class_acc_3: 0.6928 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8523 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.4660 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2656 - val_loss: 0.3486 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1750 - val_val_accuracy: 0.2180 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2418 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.4097 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.2571 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0671 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 84/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0904 - train_accuracy: 0.7286 - test_accuracy: 0.6070 - val_accuracy: 0.6120 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6364 - train_class_acc_4: 0.7500 - test_class_acc_4: 0.7651 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2344 - val_loss: 0.3478 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1860 - val_val_accuracy: 0.1820 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.3889 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.3323 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0872 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 85/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0893 - train_accuracy: 0.7500 - test_accuracy: 0.6140 - val_accuracy: 0.6020 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0769 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6489 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7181 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5146 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.3125 - val_loss: 0.3483 - val_train_accuracy: 0.0857 - val_test_accuracy: 0.1560 - val_val_accuracy: 0.1680 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.3889 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.2445 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 86/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0895 - train_accuracy: 0.7214 - test_accuracy: 0.6200 - val_accuracy: 0.6220 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6364 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8121 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2969 - val_loss: 0.3497 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1670 - val_val_accuracy: 0.1380 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.3000 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.4722 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.2508 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 87/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0889 - train_accuracy: 0.7643 - test_accuracy: 0.6370 - val_accuracy: 0.6180 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0769 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6959 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.7852 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.4660 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3125 - val_loss: 0.3480 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1580 - val_val_accuracy: 0.1460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.4236 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.2602 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0403 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 88/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0893 - train_accuracy: 0.7643 - test_accuracy: 0.6030 - val_accuracy: 0.5900 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5987 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7919 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.4951 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2812 - val_loss: 0.3489 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1490 - val_val_accuracy: 0.1680 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.3611 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.2445 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0336 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 89/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0886 - train_accuracy: 0.7571 - test_accuracy: 0.6070 - val_accuracy: 0.6260 - train_class_acc_0: 0.0000e+00 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6113 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.7919 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5243 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3438 - val_loss: 0.3484 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1780 - val_val_accuracy: 0.2040 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.5625 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.2069 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0738 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 90/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0892 - train_accuracy: 0.7500 - test_accuracy: 0.6200 - val_accuracy: 0.6240 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6520 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8054 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5049 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2031 - val_loss: 0.3478 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1750 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.3819 - val_train_class_acc_3: 0.5500 - val_test_class_acc_3: 0.3041 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 91/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0891 - train_accuracy: 0.7429 - test_accuracy: 0.6280 - val_accuracy: 0.6060 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0615 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6552 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8121 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.4757 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.2969 - val_loss: 0.3494 - val_train_accuracy: 0.1929 - val_test_accuracy: 0.2040 - val_val_accuracy: 0.2080 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.3000 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.6458 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.2571 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 92/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0889 - train_accuracy: 0.7214 - test_accuracy: 0.6160 - val_accuracy: 0.6080 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0231 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6458 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.7919 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5243 - train_class_acc_6: 0.5000 - test_class_acc_6: 0.2344 - val_loss: 0.3488 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1840 - val_val_accuracy: 0.1840 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5625 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.2602 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 93/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0888 - train_accuracy: 0.7429 - test_accuracy: 0.6230 - val_accuracy: 0.5960 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0846 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6270 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.7852 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5146 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.3438 - val_loss: 0.3488 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1650 - val_val_accuracy: 0.1560 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5000 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2351 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 94/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0888 - train_accuracy: 0.7357 - test_accuracy: 0.6220 - val_accuracy: 0.5900 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0385 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.6395 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7919 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3125 - val_loss: 0.3477 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.2150 - val_val_accuracy: 0.2180 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.5972 - val_train_class_acc_3: 0.4000 - val_test_class_acc_3: 0.3292 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0671 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 95/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0885 - train_accuracy: 0.7643 - test_accuracy: 0.6380 - val_accuracy: 0.6060 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0923 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6552 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8255 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2969 - val_loss: 0.3484 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1640 - val_val_accuracy: 0.1640 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5347 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1787 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0872 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 96/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0879 - train_accuracy: 0.7643 - test_accuracy: 0.6150 - val_accuracy: 0.5820 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0462 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6144 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.4757 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3125 - val_loss: 0.3480 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1890 - val_val_accuracy: 0.1680 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.6458 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2100 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 97/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0880 - train_accuracy: 0.7643 - test_accuracy: 0.6260 - val_accuracy: 0.6300 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.0615 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6113 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8121 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.3750 - val_loss: 0.3483 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1550 - val_val_accuracy: 0.1680 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5139 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1850 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 98/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0879 - train_accuracy: 0.7500 - test_accuracy: 0.6190 - val_accuracy: 0.6220 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0462 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6332 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8188 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.4854 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2969 - val_loss: 0.3479 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1750 - val_val_accuracy: 0.2040 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5972 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2038 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 99/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0888 - train_accuracy: 0.7500 - test_accuracy: 0.6210 - val_accuracy: 0.6260 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9167 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6332 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2812 - val_loss: 0.3492 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1800 - val_val_accuracy: 0.1780 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.5625 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.2382 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 100/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0880 - train_accuracy: 0.7500 - test_accuracy: 0.6320 - val_accuracy: 0.6260 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.1231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6050 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3594 - val_loss: 0.3472 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1770 - val_val_accuracy: 0.1560 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5625 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.2100 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 101/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0883 - train_accuracy: 0.7571 - test_accuracy: 0.6490 - val_accuracy: 0.6000 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.1000 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9792 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6520 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2812 - val_loss: 0.3476 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1790 - val_val_accuracy: 0.1600 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.6597 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1818 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1007 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 102/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0883 - train_accuracy: 0.7500 - test_accuracy: 0.6180 - val_accuracy: 0.6260 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0769 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6082 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8523 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4757 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3125 - val_loss: 0.3479 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1760 - val_val_accuracy: 0.1980 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.6319 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1850 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 103/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0880 - train_accuracy: 0.7786 - test_accuracy: 0.6110 - val_accuracy: 0.6240 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.0923 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5643 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2812 - val_loss: 0.3492 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1830 - val_val_accuracy: 0.1900 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.6250 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.2257 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.0872 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 104/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0872 - train_accuracy: 0.7429 - test_accuracy: 0.6080 - val_accuracy: 0.6380 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5956 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.7852 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.2656 - val_loss: 0.3487 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.1740 - val_val_accuracy: 0.1700 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.6389 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1724 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 105/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0873 - train_accuracy: 0.7929 - test_accuracy: 0.6040 - val_accuracy: 0.6200 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9167 - train_class_acc_3: 0.9500 - test_class_acc_3: 0.5862 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8054 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.2812 - val_loss: 0.3483 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1500 - val_val_accuracy: 0.1720 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.5000 - val_train_class_acc_3: 0.3500 - val_test_class_acc_3: 0.1881 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 106/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0871 - train_accuracy: 0.7929 - test_accuracy: 0.5980 - val_accuracy: 0.6200 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.0385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8462 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5705 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8255 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5243 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.3125 - val_loss: 0.3462 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1830 - val_val_accuracy: 0.1820 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5417 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2414 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 107/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0872 - train_accuracy: 0.7714 - test_accuracy: 0.6250 - val_accuracy: 0.6080 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0923 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6113 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3125 - val_loss: 0.3469 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1620 - val_val_accuracy: 0.1540 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.6111 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1818 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0604 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 108/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0877 - train_accuracy: 0.7786 - test_accuracy: 0.6180 - val_accuracy: 0.6000 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6113 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8188 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.2812 - val_loss: 0.3476 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1610 - val_val_accuracy: 0.1500 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.4444 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2163 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 109/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0870 - train_accuracy: 0.7429 - test_accuracy: 0.6170 - val_accuracy: 0.6040 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6144 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.3125 - val_loss: 0.3480 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.2000 - val_val_accuracy: 0.2140 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.6319 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.2414 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1275 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 110/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0867 - train_accuracy: 0.7786 - test_accuracy: 0.6270 - val_accuracy: 0.6220 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.0769 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6332 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2656 - val_loss: 0.3476 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1840 - val_val_accuracy: 0.2000 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5556 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2194 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.1611 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 111/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0873 - train_accuracy: 0.7643 - test_accuracy: 0.6170 - val_accuracy: 0.6260 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.0615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6019 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.4854 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.3281 - val_loss: 0.3470 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1690 - val_val_accuracy: 0.1720 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5556 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1850 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 112/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0865 - train_accuracy: 0.7929 - test_accuracy: 0.6260 - val_accuracy: 0.6340 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.0538 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6144 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8255 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3438 - val_loss: 0.3471 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1350 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1567 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0671 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 113/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0871 - train_accuracy: 0.7643 - test_accuracy: 0.6320 - val_accuracy: 0.6120 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.1231 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5925 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.3125 - val_loss: 0.3466 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.1760 - val_val_accuracy: 0.1640 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.6528 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1693 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 114/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0872 - train_accuracy: 0.7786 - test_accuracy: 0.6140 - val_accuracy: 0.6120 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.0846 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5799 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8255 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5243 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3125 - val_loss: 0.3484 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1810 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.6250 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1693 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 115/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0864 - train_accuracy: 0.7643 - test_accuracy: 0.6200 - val_accuracy: 0.6040 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0462 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5799 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3438 - val_loss: 0.3478 - val_train_accuracy: 0.1929 - val_test_accuracy: 0.1670 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.6319 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1505 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.0470 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 116/200\n",
            "2708/2708 [==============================] - 0s 28us/sample - loss: 0.0863 - train_accuracy: 0.7714 - test_accuracy: 0.6200 - val_accuracy: 0.6160 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.1154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5643 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.2812 - val_loss: 0.3485 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1600 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5625 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1630 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 117/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0858 - train_accuracy: 0.7714 - test_accuracy: 0.6060 - val_accuracy: 0.6000 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0462 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5956 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8255 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5049 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.2500 - val_loss: 0.3475 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1800 - val_val_accuracy: 0.1580 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.6250 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.2038 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 118/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0864 - train_accuracy: 0.7429 - test_accuracy: 0.6320 - val_accuracy: 0.6200 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.1077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9792 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5987 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.3438 - val_loss: 0.3470 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1540 - val_val_accuracy: 0.1720 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5486 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1661 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 119/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0861 - train_accuracy: 0.7571 - test_accuracy: 0.6340 - val_accuracy: 0.6480 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.1077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.6019 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.3281 - val_loss: 0.3473 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1760 - val_val_accuracy: 0.1860 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.6042 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1787 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 120/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0861 - train_accuracy: 0.7714 - test_accuracy: 0.6360 - val_accuracy: 0.6120 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.1385 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6019 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.3750 - val_loss: 0.3470 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1760 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.6458 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1850 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 121/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0867 - train_accuracy: 0.7500 - test_accuracy: 0.6130 - val_accuracy: 0.6000 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.1231 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5705 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.9000 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.5500 - test_class_acc_6: 0.3125 - val_loss: 0.3456 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1820 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.5347 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2351 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 122/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0848 - train_accuracy: 0.7286 - test_accuracy: 0.6270 - val_accuracy: 0.6360 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.1077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5893 - train_class_acc_4: 0.8500 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2656 - val_loss: 0.3469 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1810 - val_val_accuracy: 0.1980 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.6597 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1850 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 123/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0863 - train_accuracy: 0.7786 - test_accuracy: 0.6210 - val_accuracy: 0.6220 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.1231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5862 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2969 - val_loss: 0.3464 - val_train_accuracy: 0.1071 - val_test_accuracy: 0.1700 - val_val_accuracy: 0.1840 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.5000 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.2100 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1007 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 124/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0853 - train_accuracy: 0.7857 - test_accuracy: 0.6350 - val_accuracy: 0.6260 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.1385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5862 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3906 - val_loss: 0.3478 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.1380 - val_val_accuracy: 0.1260 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0659 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4792 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1473 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1007 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 125/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0857 - train_accuracy: 0.7786 - test_accuracy: 0.6370 - val_accuracy: 0.6180 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.1077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6019 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.4375 - val_loss: 0.3472 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1760 - val_val_accuracy: 0.1500 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5417 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.2163 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 126/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0853 - train_accuracy: 0.7786 - test_accuracy: 0.6040 - val_accuracy: 0.6040 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.0692 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5611 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8255 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.2812 - val_loss: 0.3454 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1450 - val_val_accuracy: 0.1380 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.5000 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1411 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0872 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 127/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0855 - train_accuracy: 0.7929 - test_accuracy: 0.6260 - val_accuracy: 0.6280 - train_class_acc_0: 0.3000 - test_class_acc_0: 0.2077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5674 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.2812 - val_loss: 0.3479 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1610 - val_val_accuracy: 0.1780 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1209 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4931 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1599 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1678 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0156\n",
            "Epoch 128/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0859 - train_accuracy: 0.7643 - test_accuracy: 0.6270 - val_accuracy: 0.6160 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.1462 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5737 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.3281 - val_loss: 0.3480 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1810 - val_val_accuracy: 0.1680 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.6389 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1724 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1544 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 129/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0847 - train_accuracy: 0.7643 - test_accuracy: 0.6170 - val_accuracy: 0.6260 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.1000 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5643 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.6500 - test_class_acc_6: 0.3281 - val_loss: 0.3477 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1930 - val_val_accuracy: 0.1920 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.2198 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.6319 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1975 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.1208 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 130/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0856 - train_accuracy: 0.7643 - test_accuracy: 0.6120 - val_accuracy: 0.6200 - train_class_acc_0: 0.0500 - test_class_acc_0: 0.0846 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5768 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.3438 - val_loss: 0.3474 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.1840 - val_val_accuracy: 0.1880 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.6667 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1724 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1275 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 131/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0862 - train_accuracy: 0.7786 - test_accuracy: 0.6400 - val_accuracy: 0.6160 - train_class_acc_0: 0.3000 - test_class_acc_0: 0.1308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5799 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.9060 - train_class_acc_5: 0.6500 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4375 - val_loss: 0.3476 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.2050 - val_val_accuracy: 0.1680 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.7431 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1787 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1678 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 132/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0855 - train_accuracy: 0.8143 - test_accuracy: 0.6110 - val_accuracy: 0.6040 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.1538 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5455 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5146 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3281 - val_loss: 0.3477 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1850 - val_val_accuracy: 0.1980 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2747 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.6597 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1599 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 133/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0858 - train_accuracy: 0.8071 - test_accuracy: 0.6210 - val_accuracy: 0.6140 - train_class_acc_0: 0.4000 - test_class_acc_0: 0.2077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5705 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.7919 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5049 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.3281 - val_loss: 0.3471 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1760 - val_val_accuracy: 0.1580 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.6181 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1661 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 134/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0845 - train_accuracy: 0.7357 - test_accuracy: 0.6400 - val_accuracy: 0.6100 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.1462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.5862 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5728 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.4219 - val_loss: 0.3480 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.1800 - val_val_accuracy: 0.1840 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.6667 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1661 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 135/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0845 - train_accuracy: 0.7571 - test_accuracy: 0.6350 - val_accuracy: 0.6120 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.2154 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.5705 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5437 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4062 - val_loss: 0.3474 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1740 - val_val_accuracy: 0.1780 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5903 - val_train_class_acc_3: 0.0000e+00 - val_test_class_acc_3: 0.1536 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1812 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 136/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0840 - train_accuracy: 0.8286 - test_accuracy: 0.6370 - val_accuracy: 0.6280 - train_class_acc_0: 0.3500 - test_class_acc_0: 0.2077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5643 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8926 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5728 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.3906 - val_loss: 0.3477 - val_train_accuracy: 0.2000 - val_test_accuracy: 0.1840 - val_val_accuracy: 0.2360 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.8500 - val_test_class_acc_2: 0.6389 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1567 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1544 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 137/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0840 - train_accuracy: 0.7929 - test_accuracy: 0.6270 - val_accuracy: 0.6160 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.1692 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5831 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8054 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.4062 - val_loss: 0.3473 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1790 - val_val_accuracy: 0.2040 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.2747 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.5000 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1630 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.2013 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 138/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0852 - train_accuracy: 0.8143 - test_accuracy: 0.6400 - val_accuracy: 0.6380 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.1385 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6207 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.3906 - val_loss: 0.3481 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1670 - val_val_accuracy: 0.1560 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.6458 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1411 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0738 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 139/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0850 - train_accuracy: 0.8000 - test_accuracy: 0.6370 - val_accuracy: 0.6380 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.2077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5580 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8725 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4375 - val_loss: 0.3465 - val_train_accuracy: 0.0714 - val_test_accuracy: 0.1600 - val_val_accuracy: 0.1620 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.2000 - val_test_class_acc_2: 0.5903 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1536 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 140/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0846 - train_accuracy: 0.8214 - test_accuracy: 0.6280 - val_accuracy: 0.6260 - train_class_acc_0: 0.4000 - test_class_acc_0: 0.1538 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5799 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8523 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3750 - val_loss: 0.3475 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1780 - val_val_accuracy: 0.1780 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0000e+00 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.6597 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1724 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 141/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0846 - train_accuracy: 0.7571 - test_accuracy: 0.6270 - val_accuracy: 0.6140 - train_class_acc_0: 0.1000 - test_class_acc_0: 0.1385 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5737 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8255 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.4062 - val_loss: 0.3465 - val_train_accuracy: 0.2071 - val_test_accuracy: 0.1910 - val_val_accuracy: 0.2320 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.2308 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.6528 - val_train_class_acc_3: 0.4500 - val_test_class_acc_3: 0.1944 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.1000 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 142/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0843 - train_accuracy: 0.7929 - test_accuracy: 0.6320 - val_accuracy: 0.6260 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.1308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5705 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5728 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.3906 - val_loss: 0.3459 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1620 - val_val_accuracy: 0.1700 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2308 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1599 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1678 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 143/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0841 - train_accuracy: 0.8000 - test_accuracy: 0.6380 - val_accuracy: 0.6340 - train_class_acc_0: 0.2000 - test_class_acc_0: 0.1385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.6082 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8121 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4219 - val_loss: 0.3467 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1590 - val_val_accuracy: 0.1760 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.2198 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5764 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1129 - val_train_class_acc_4: 0.3500 - val_test_class_acc_4: 0.1342 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 144/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0839 - train_accuracy: 0.8000 - test_accuracy: 0.6360 - val_accuracy: 0.6120 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.1462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5705 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8993 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4062 - val_loss: 0.3464 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1580 - val_val_accuracy: 0.1660 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.2527 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5069 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1536 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 145/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0838 - train_accuracy: 0.8143 - test_accuracy: 0.6360 - val_accuracy: 0.6420 - train_class_acc_0: 0.3500 - test_class_acc_0: 0.1692 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5580 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4375 - val_loss: 0.3469 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1960 - val_val_accuracy: 0.1680 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2308 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5208 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.2163 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1946 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 146/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0845 - train_accuracy: 0.7571 - test_accuracy: 0.6350 - val_accuracy: 0.5920 - train_class_acc_0: 0.1500 - test_class_acc_0: 0.1769 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5768 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3750 - val_loss: 0.3466 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.1300 - val_val_accuracy: 0.1180 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.3000 - val_test_class_acc_2: 0.4097 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1442 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 147/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0840 - train_accuracy: 0.8357 - test_accuracy: 0.6540 - val_accuracy: 0.6260 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.2308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5674 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8993 - train_class_acc_5: 0.9000 - test_class_acc_5: 0.6408 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.3594 - val_loss: 0.3467 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1810 - val_val_accuracy: 0.1700 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.5694 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1881 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1409 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 148/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0838 - train_accuracy: 0.8071 - test_accuracy: 0.6470 - val_accuracy: 0.6380 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.1846 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5674 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.9060 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5728 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4062 - val_loss: 0.3464 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1640 - val_val_accuracy: 0.1600 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.4931 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1818 - val_train_class_acc_4: 0.2500 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 149/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0840 - train_accuracy: 0.8214 - test_accuracy: 0.6420 - val_accuracy: 0.6400 - train_class_acc_0: 0.4000 - test_class_acc_0: 0.2154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5768 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4531 - val_loss: 0.3467 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.1750 - val_val_accuracy: 0.1720 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5347 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1787 - val_train_class_acc_4: 0.3000 - val_test_class_acc_4: 0.1611 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 150/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0834 - train_accuracy: 0.8000 - test_accuracy: 0.6320 - val_accuracy: 0.6280 - train_class_acc_0: 0.3000 - test_class_acc_0: 0.2077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5392 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4531 - val_loss: 0.3470 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.1420 - val_val_accuracy: 0.1540 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.3000 - val_test_class_acc_1: 0.0659 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1536 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1208 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0312\n",
            "Epoch 151/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0848 - train_accuracy: 0.7643 - test_accuracy: 0.6330 - val_accuracy: 0.6320 - train_class_acc_0: 0.2500 - test_class_acc_0: 0.1462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5643 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.9060 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5340 - train_class_acc_6: 0.6000 - test_class_acc_6: 0.3750 - val_loss: 0.3463 - val_train_accuracy: 0.1857 - val_test_accuracy: 0.1490 - val_val_accuracy: 0.1660 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.5000 - val_test_class_acc_1: 0.2637 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.3750 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1473 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1544 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 152/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0837 - train_accuracy: 0.8571 - test_accuracy: 0.6570 - val_accuracy: 0.6340 - train_class_acc_0: 0.4000 - test_class_acc_0: 0.2923 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.5831 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4219 - val_loss: 0.3474 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1660 - val_val_accuracy: 0.1920 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2198 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.5139 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1473 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1678 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 153/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0836 - train_accuracy: 0.7929 - test_accuracy: 0.6540 - val_accuracy: 0.6520 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.2154 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.6000 - test_class_acc_3: 0.5737 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.5000 - val_loss: 0.3477 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1480 - val_val_accuracy: 0.1500 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0659 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5833 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1285 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1007 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 154/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0828 - train_accuracy: 0.8357 - test_accuracy: 0.6520 - val_accuracy: 0.6380 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.2846 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5674 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4531 - val_loss: 0.3460 - val_train_accuracy: 0.1000 - val_test_accuracy: 0.1580 - val_val_accuracy: 0.1320 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.4583 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1411 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1812 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 155/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0827 - train_accuracy: 0.8000 - test_accuracy: 0.6600 - val_accuracy: 0.6360 - train_class_acc_0: 0.3000 - test_class_acc_0: 0.2308 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.5768 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8993 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4531 - val_loss: 0.3458 - val_train_accuracy: 0.0929 - val_test_accuracy: 0.1720 - val_val_accuracy: 0.1660 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5417 - val_train_class_acc_3: 0.0000e+00 - val_test_class_acc_3: 0.1630 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.1342 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 156/200\n",
            "2708/2708 [==============================] - 0s 29us/sample - loss: 0.0839 - train_accuracy: 0.8357 - test_accuracy: 0.6350 - val_accuracy: 0.6360 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.2923 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8889 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5235 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8859 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.3906 - val_loss: 0.3467 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1990 - val_val_accuracy: 0.1920 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.7014 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1818 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1342 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 157/200\n",
            "2708/2708 [==============================] - 0s 30us/sample - loss: 0.0830 - train_accuracy: 0.8357 - test_accuracy: 0.6580 - val_accuracy: 0.6500 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.3462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5799 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4375 - val_loss: 0.3461 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1630 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5972 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1411 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.0872 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 158/200\n",
            "2708/2708 [==============================] - 0s 32us/sample - loss: 0.0831 - train_accuracy: 0.8429 - test_accuracy: 0.6500 - val_accuracy: 0.6260 - train_class_acc_0: 0.6000 - test_class_acc_0: 0.3154 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5455 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8188 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4844 - val_loss: 0.3474 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1510 - val_val_accuracy: 0.1460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2527 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5278 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1129 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0388 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 159/200\n",
            "2708/2708 [==============================] - 0s 30us/sample - loss: 0.0823 - train_accuracy: 0.8000 - test_accuracy: 0.6620 - val_accuracy: 0.6680 - train_class_acc_0: 0.3000 - test_class_acc_0: 0.2538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6176 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.7000 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5156 - val_loss: 0.3449 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1770 - val_val_accuracy: 0.1660 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.2198 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.6319 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1693 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0671 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 160/200\n",
            "2708/2708 [==============================] - 0s 36us/sample - loss: 0.0828 - train_accuracy: 0.8357 - test_accuracy: 0.6450 - val_accuracy: 0.6500 - train_class_acc_0: 0.4000 - test_class_acc_0: 0.1846 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5737 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.9060 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.5534 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4375 - val_loss: 0.3466 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1350 - val_val_accuracy: 0.1280 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.4236 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1348 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1544 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 161/200\n",
            "2708/2708 [==============================] - 0s 31us/sample - loss: 0.0833 - train_accuracy: 0.8286 - test_accuracy: 0.6400 - val_accuracy: 0.6300 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.2538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.8958 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5580 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8322 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5156 - val_loss: 0.3477 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1580 - val_val_accuracy: 0.1160 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5208 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1661 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1409 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 162/200\n",
            "2708/2708 [==============================] - 0s 32us/sample - loss: 0.0828 - train_accuracy: 0.8000 - test_accuracy: 0.6370 - val_accuracy: 0.6440 - train_class_acc_0: 0.3500 - test_class_acc_0: 0.2000 - train_class_acc_1: 0.9000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5799 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4062 - val_loss: 0.3462 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1730 - val_val_accuracy: 0.1580 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.5972 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1599 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 163/200\n",
            "2708/2708 [==============================] - 0s 28us/sample - loss: 0.0824 - train_accuracy: 0.8714 - test_accuracy: 0.6590 - val_accuracy: 0.6680 - train_class_acc_0: 0.5500 - test_class_acc_0: 0.3077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5705 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8523 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5000 - val_loss: 0.3448 - val_train_accuracy: 0.2071 - val_test_accuracy: 0.1670 - val_val_accuracy: 0.2120 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2088 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.5625 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1505 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 164/200\n",
            "2708/2708 [==============================] - 0s 28us/sample - loss: 0.0821 - train_accuracy: 0.8286 - test_accuracy: 0.6610 - val_accuracy: 0.6440 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.3385 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.7000 - test_class_acc_3: 0.5737 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5728 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4375 - val_loss: 0.3460 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.1830 - val_val_accuracy: 0.1740 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2967 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5833 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1536 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1477 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 165/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0817 - train_accuracy: 0.8643 - test_accuracy: 0.6740 - val_accuracy: 0.6580 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.3231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6050 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4844 - val_loss: 0.3446 - val_train_accuracy: 0.1571 - val_test_accuracy: 0.1630 - val_val_accuracy: 0.1560 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2308 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.4861 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1661 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1141 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 166/200\n",
            "2708/2708 [==============================] - 0s 29us/sample - loss: 0.0823 - train_accuracy: 0.8571 - test_accuracy: 0.6770 - val_accuracy: 0.6820 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.2923 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8571 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6238 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8859 - train_class_acc_5: 0.9000 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4844 - val_loss: 0.3477 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1750 - val_val_accuracy: 0.1820 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.6528 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1411 - val_train_class_acc_4: 0.2500 - val_test_class_acc_4: 0.1208 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0388 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 167/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0824 - train_accuracy: 0.8143 - test_accuracy: 0.6650 - val_accuracy: 0.6820 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.3615 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5799 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8121 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.5000 - val_loss: 0.3474 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1510 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1538 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.5069 - val_train_class_acc_3: 0.3000 - val_test_class_acc_3: 0.1285 - val_train_class_acc_4: 0.3000 - val_test_class_acc_4: 0.1409 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 168/200\n",
            "2708/2708 [==============================] - 0s 31us/sample - loss: 0.0826 - train_accuracy: 0.8286 - test_accuracy: 0.6610 - val_accuracy: 0.6540 - train_class_acc_0: 0.3500 - test_class_acc_0: 0.2538 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5705 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8523 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.6094 - val_loss: 0.3471 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1450 - val_val_accuracy: 0.1600 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.4861 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1567 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.0872 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0156\n",
            "Epoch 169/200\n",
            "2708/2708 [==============================] - 0s 27us/sample - loss: 0.0826 - train_accuracy: 0.8143 - test_accuracy: 0.6630 - val_accuracy: 0.6640 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.3308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5580 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8523 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.5156 - val_loss: 0.3471 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1610 - val_val_accuracy: 0.1420 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5278 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1661 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1409 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 170/200\n",
            "2708/2708 [==============================] - 0s 28us/sample - loss: 0.0826 - train_accuracy: 0.8214 - test_accuracy: 0.6710 - val_accuracy: 0.6580 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.3462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9167 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5862 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.9128 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4688 - val_loss: 0.3455 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1610 - val_val_accuracy: 0.1400 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.2198 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.4722 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1285 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.2081 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 171/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0838 - train_accuracy: 0.8357 - test_accuracy: 0.6600 - val_accuracy: 0.6680 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.2692 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9653 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5674 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6311 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4219 - val_loss: 0.3471 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1370 - val_val_accuracy: 0.1360 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5208 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1317 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.0805 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 172/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0824 - train_accuracy: 0.8286 - test_accuracy: 0.6570 - val_accuracy: 0.6460 - train_class_acc_0: 0.6000 - test_class_acc_0: 0.3538 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5361 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.6311 - train_class_acc_6: 0.7000 - test_class_acc_6: 0.4219 - val_loss: 0.3458 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1680 - val_val_accuracy: 0.1800 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.6319 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1160 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1879 - val_train_class_acc_5: 0.1000 - val_test_class_acc_5: 0.0388 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 173/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0820 - train_accuracy: 0.8643 - test_accuracy: 0.6690 - val_accuracy: 0.6380 - train_class_acc_0: 0.6000 - test_class_acc_0: 0.2769 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5925 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8926 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5156 - val_loss: 0.3441 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.2000 - val_val_accuracy: 0.2060 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.7083 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1473 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.2081 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0388 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 174/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0826 - train_accuracy: 0.8429 - test_accuracy: 0.6700 - val_accuracy: 0.6700 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.3615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5799 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6214 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4531 - val_loss: 0.3473 - val_train_accuracy: 0.1214 - val_test_accuracy: 0.1480 - val_val_accuracy: 0.1280 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.5139 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1034 - val_train_class_acc_4: 0.0000e+00 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0388 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 175/200\n",
            "2708/2708 [==============================] - 0s 31us/sample - loss: 0.0811 - train_accuracy: 0.8500 - test_accuracy: 0.6570 - val_accuracy: 0.6660 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.2615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5611 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.6408 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5625 - val_loss: 0.3456 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1680 - val_val_accuracy: 0.1780 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5486 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1379 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1611 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 176/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0812 - train_accuracy: 0.8357 - test_accuracy: 0.6630 - val_accuracy: 0.6800 - train_class_acc_0: 0.6000 - test_class_acc_0: 0.3615 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5549 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.5156 - val_loss: 0.3460 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1620 - val_val_accuracy: 0.1480 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.5833 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1473 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1275 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 177/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0827 - train_accuracy: 0.8429 - test_accuracy: 0.6930 - val_accuracy: 0.6680 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.4692 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5674 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.9128 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5312 - val_loss: 0.3462 - val_train_accuracy: 0.1929 - val_test_accuracy: 0.1710 - val_val_accuracy: 0.1720 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.5764 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1599 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1275 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 178/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0824 - train_accuracy: 0.8357 - test_accuracy: 0.6760 - val_accuracy: 0.6720 - train_class_acc_0: 0.5500 - test_class_acc_0: 0.4000 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5768 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8859 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6214 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4844 - val_loss: 0.3463 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1560 - val_val_accuracy: 0.1480 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5903 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1285 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.0940 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 179/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0813 - train_accuracy: 0.8571 - test_accuracy: 0.6790 - val_accuracy: 0.6660 - train_class_acc_0: 0.5500 - test_class_acc_0: 0.3462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5956 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5469 - val_loss: 0.3466 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1550 - val_val_accuracy: 0.1840 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.5486 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1317 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1208 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0156\n",
            "Epoch 180/200\n",
            "2708/2708 [==============================] - 0s 26us/sample - loss: 0.0818 - train_accuracy: 0.8571 - test_accuracy: 0.6850 - val_accuracy: 0.6700 - train_class_acc_0: 0.5500 - test_class_acc_0: 0.4692 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5737 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8591 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5312 - val_loss: 0.3456 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1760 - val_val_accuracy: 0.1440 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.5500 - val_test_class_acc_2: 0.4861 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1850 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.2349 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0156\n",
            "Epoch 181/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0814 - train_accuracy: 0.8643 - test_accuracy: 0.6690 - val_accuracy: 0.6540 - train_class_acc_0: 0.6500 - test_class_acc_0: 0.3308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5768 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8725 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.6311 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.4531 - val_loss: 0.3455 - val_train_accuracy: 0.1071 - val_test_accuracy: 0.1500 - val_val_accuracy: 0.1760 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5417 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1191 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1275 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 182/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0823 - train_accuracy: 0.8429 - test_accuracy: 0.6790 - val_accuracy: 0.6600 - train_class_acc_0: 0.6000 - test_class_acc_0: 0.4462 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5831 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6311 - train_class_acc_6: 0.7500 - test_class_acc_6: 0.4531 - val_loss: 0.3477 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1780 - val_val_accuracy: 0.1660 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.6500 - val_test_class_acc_2: 0.5556 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1567 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.2081 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 183/200\n",
            "2708/2708 [==============================] - 0s 23us/sample - loss: 0.0812 - train_accuracy: 0.8571 - test_accuracy: 0.6760 - val_accuracy: 0.6840 - train_class_acc_0: 0.6500 - test_class_acc_0: 0.4077 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9722 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5643 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5625 - val_loss: 0.3462 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1490 - val_val_accuracy: 0.1420 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.1099 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.4375 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1850 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1074 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 184/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0812 - train_accuracy: 0.8429 - test_accuracy: 0.6800 - val_accuracy: 0.6580 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.3692 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5831 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.9060 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5156 - val_loss: 0.3457 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1330 - val_val_accuracy: 0.1380 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0308 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.7000 - val_test_class_acc_2: 0.4167 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1129 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1477 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 185/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0811 - train_accuracy: 0.8429 - test_accuracy: 0.6640 - val_accuracy: 0.6640 - train_class_acc_0: 0.4500 - test_class_acc_0: 0.3462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5674 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5922 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5000 - val_loss: 0.3460 - val_train_accuracy: 0.1714 - val_test_accuracy: 0.1680 - val_val_accuracy: 0.1880 - val_train_class_acc_0: 0.1000 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0879 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5764 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1442 - val_train_class_acc_4: 0.3000 - val_test_class_acc_4: 0.1946 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 186/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0812 - train_accuracy: 0.8571 - test_accuracy: 0.6790 - val_accuracy: 0.6880 - train_class_acc_0: 0.5000 - test_class_acc_0: 0.4000 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9167 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5831 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5728 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5625 - val_loss: 0.3439 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1560 - val_val_accuracy: 0.1420 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0231 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.0440 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5833 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1348 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1342 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0156\n",
            "Epoch 187/200\n",
            "2708/2708 [==============================] - 0s 22us/sample - loss: 0.0814 - train_accuracy: 0.8643 - test_accuracy: 0.6620 - val_accuracy: 0.6760 - train_class_acc_0: 0.7000 - test_class_acc_0: 0.3615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 0.9500 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5549 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8658 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.6602 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4688 - val_loss: 0.3475 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1670 - val_val_accuracy: 0.1600 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1429 - val_train_class_acc_2: 0.6000 - val_test_class_acc_2: 0.5556 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1505 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1544 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 188/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0806 - train_accuracy: 0.8643 - test_accuracy: 0.6860 - val_accuracy: 0.7040 - train_class_acc_0: 0.6500 - test_class_acc_0: 0.4077 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5893 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8523 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.6311 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5000 - val_loss: 0.3467 - val_train_accuracy: 0.1643 - val_test_accuracy: 0.1750 - val_val_accuracy: 0.1840 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1868 - val_train_class_acc_2: 0.7500 - val_test_class_acc_2: 0.6111 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1160 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.2013 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 189/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0806 - train_accuracy: 0.8857 - test_accuracy: 0.6900 - val_accuracy: 0.6660 - train_class_acc_0: 0.7000 - test_class_acc_0: 0.4462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.5987 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8859 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5156 - val_loss: 0.3461 - val_train_accuracy: 0.1429 - val_test_accuracy: 0.1790 - val_val_accuracy: 0.1820 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.2418 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.5347 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1442 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.2013 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 190/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0807 - train_accuracy: 0.8500 - test_accuracy: 0.6760 - val_accuracy: 0.6780 - train_class_acc_0: 0.6500 - test_class_acc_0: 0.4231 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9231 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9583 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5862 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8188 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6019 - train_class_acc_6: 0.8000 - test_class_acc_6: 0.4375 - val_loss: 0.3442 - val_train_accuracy: 0.1786 - val_test_accuracy: 0.1600 - val_val_accuracy: 0.1640 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2500 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5278 - val_train_class_acc_3: 0.2500 - val_test_class_acc_3: 0.1818 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1007 - val_train_class_acc_5: 0.0500 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 191/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0799 - train_accuracy: 0.8500 - test_accuracy: 0.6800 - val_accuracy: 0.6740 - train_class_acc_0: 0.7000 - test_class_acc_0: 0.4385 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9341 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9236 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5580 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8725 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5825 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5781 - val_loss: 0.3461 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1400 - val_val_accuracy: 0.1640 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4653 - val_train_class_acc_3: 0.2000 - val_test_class_acc_3: 0.1223 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1611 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 192/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0806 - train_accuracy: 0.9000 - test_accuracy: 0.7140 - val_accuracy: 0.6740 - train_class_acc_0: 0.7000 - test_class_acc_0: 0.4923 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9375 - train_class_acc_3: 0.9000 - test_class_acc_3: 0.6270 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.9262 - train_class_acc_5: 0.8500 - test_class_acc_5: 0.5631 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5625 - val_loss: 0.3464 - val_train_accuracy: 0.0857 - val_test_accuracy: 0.1290 - val_val_accuracy: 0.1460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0154 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.4306 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1160 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1275 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 193/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0803 - train_accuracy: 0.8714 - test_accuracy: 0.6880 - val_accuracy: 0.6900 - train_class_acc_0: 0.7500 - test_class_acc_0: 0.4692 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5737 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.5728 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5781 - val_loss: 0.3460 - val_train_accuracy: 0.1357 - val_test_accuracy: 0.1670 - val_val_accuracy: 0.1400 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1758 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5069 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1661 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1544 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 194/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0797 - train_accuracy: 0.8429 - test_accuracy: 0.6860 - val_accuracy: 0.6780 - train_class_acc_0: 0.6000 - test_class_acc_0: 0.4538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9514 - train_class_acc_3: 0.7500 - test_class_acc_3: 0.5517 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.9060 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5156 - val_loss: 0.3473 - val_train_accuracy: 0.1286 - val_test_accuracy: 0.1530 - val_val_accuracy: 0.1440 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0077 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.1319 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.4444 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1411 - val_train_class_acc_4: 0.1500 - val_test_class_acc_4: 0.1879 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 195/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0809 - train_accuracy: 0.8714 - test_accuracy: 0.7010 - val_accuracy: 0.6760 - train_class_acc_0: 0.6500 - test_class_acc_0: 0.4462 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.9121 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9167 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6050 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8926 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6214 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.5938 - val_loss: 0.3463 - val_train_accuracy: 0.1500 - val_test_accuracy: 0.1500 - val_val_accuracy: 0.1460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.3000 - val_test_class_acc_1: 0.2308 - val_train_class_acc_2: 0.3500 - val_test_class_acc_2: 0.4514 - val_train_class_acc_3: 0.1000 - val_test_class_acc_3: 0.1348 - val_train_class_acc_4: 0.3000 - val_test_class_acc_4: 0.1342 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 196/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0808 - train_accuracy: 0.8571 - test_accuracy: 0.6910 - val_accuracy: 0.6640 - train_class_acc_0: 0.6000 - test_class_acc_0: 0.4615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8791 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9306 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5831 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8792 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5781 - val_loss: 0.3442 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1640 - val_val_accuracy: 0.1460 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0231 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1978 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.4792 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1630 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1477 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 197/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0797 - train_accuracy: 0.8786 - test_accuracy: 0.6920 - val_accuracy: 0.6820 - train_class_acc_0: 0.6500 - test_class_acc_0: 0.4846 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5705 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8859 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6311 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.6250 - val_loss: 0.3449 - val_train_accuracy: 0.1071 - val_test_accuracy: 0.1560 - val_val_accuracy: 0.1600 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1500 - val_test_class_acc_1: 0.0769 - val_train_class_acc_2: 0.5000 - val_test_class_acc_2: 0.5000 - val_train_class_acc_3: 0.0500 - val_test_class_acc_3: 0.1755 - val_train_class_acc_4: 0.0500 - val_test_class_acc_4: 0.1342 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0097 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 198/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0796 - train_accuracy: 0.8786 - test_accuracy: 0.6910 - val_accuracy: 0.6680 - train_class_acc_0: 0.7500 - test_class_acc_0: 0.4538 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.9011 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9444 - train_class_acc_3: 0.8500 - test_class_acc_3: 0.6082 - train_class_acc_4: 0.9500 - test_class_acc_4: 0.8389 - train_class_acc_5: 0.8000 - test_class_acc_5: 0.6117 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5000 - val_loss: 0.3468 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1310 - val_val_accuracy: 0.1700 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.1000 - val_test_class_acc_1: 0.0989 - val_train_class_acc_2: 0.4000 - val_test_class_acc_2: 0.4583 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1097 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1409 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0000e+00 - val_train_class_acc_6: 0.0500 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 199/200\n",
            "2708/2708 [==============================] - 0s 25us/sample - loss: 0.0806 - train_accuracy: 0.8500 - test_accuracy: 0.7020 - val_accuracy: 0.7120 - train_class_acc_0: 0.6500 - test_class_acc_0: 0.5308 - train_class_acc_1: 0.9500 - test_class_acc_1: 0.8901 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9097 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.5925 - train_class_acc_4: 0.9000 - test_class_acc_4: 0.8456 - train_class_acc_5: 0.7500 - test_class_acc_5: 0.6311 - train_class_acc_6: 0.9000 - test_class_acc_6: 0.6406 - val_loss: 0.3467 - val_train_accuracy: 0.1000 - val_test_accuracy: 0.1400 - val_val_accuracy: 0.1400 - val_train_class_acc_0: 0.0000e+00 - val_test_class_acc_0: 0.0000e+00 - val_train_class_acc_1: 0.2000 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.1500 - val_test_class_acc_2: 0.4236 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1223 - val_train_class_acc_4: 0.2000 - val_test_class_acc_4: 0.1477 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0291 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Epoch 200/200\n",
            "2708/2708 [==============================] - 0s 24us/sample - loss: 0.0787 - train_accuracy: 0.9000 - test_accuracy: 0.7190 - val_accuracy: 0.6920 - train_class_acc_0: 0.7500 - test_class_acc_0: 0.5615 - train_class_acc_1: 1.0000 - test_class_acc_1: 0.8681 - train_class_acc_2: 1.0000 - test_class_acc_2: 0.9167 - train_class_acc_3: 0.8000 - test_class_acc_3: 0.6332 - train_class_acc_4: 1.0000 - test_class_acc_4: 0.8725 - train_class_acc_5: 0.9000 - test_class_acc_5: 0.6602 - train_class_acc_6: 0.8500 - test_class_acc_6: 0.5469 - val_loss: 0.3446 - val_train_accuracy: 0.1143 - val_test_accuracy: 0.1620 - val_val_accuracy: 0.1540 - val_train_class_acc_0: 0.0500 - val_test_class_acc_0: 0.0308 - val_train_class_acc_1: 0.0500 - val_test_class_acc_1: 0.1648 - val_train_class_acc_2: 0.4500 - val_test_class_acc_2: 0.5556 - val_train_class_acc_3: 0.1500 - val_test_class_acc_3: 0.1097 - val_train_class_acc_4: 0.1000 - val_test_class_acc_4: 0.1745 - val_train_class_acc_5: 0.0000e+00 - val_test_class_acc_5: 0.0194 - val_train_class_acc_6: 0.0000e+00 - val_test_class_acc_6: 0.0000e+00\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gcn_layer_3 (GCNLayer)       multiple                  22928     \n",
            "_________________________________________________________________\n",
            "gcn_layer_4 (GCNLayer)       multiple                  112       \n",
            "=================================================================\n",
            "Total params: 23,040\n",
            "Trainable params: 23,040\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqikApUyF_HE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "00909819-f84c-4832-8035-31b552112185"
      },
      "source": [
        "# Display graphs of how the model performed\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "\n",
        "def smooth(key):\n",
        "  return gaussian_filter1d(history.history[key], sigma=2)\n",
        "\n",
        "print(\"Final loss: \", history.history['loss'][-1])\n",
        "print(\"Final training accuracy: \", round(history.history['train_accuracy'][-1]*100), \"%\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [15, 10]\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title(\"Training loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(smooth('train_accuracy'))\n",
        "plt.title(\"Training accuracy\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(smooth('val_accuracy'))\n",
        "plt.title(\"Validation accuracy\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "for label in CLASS_LABELS:\n",
        "  plt.plot(smooth(\"train_class_acc_\"+str(label)))\n",
        "plt.title(\"Training accuracy by class\")\n",
        "plt.ylabel('Class accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['class_'+str(label) for label in CLASS_LABELS], loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final loss:  0.0787268579006195\n",
            "Final training accuracy:  90.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAALICAYAAACJhQBYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGJIkSZIkqfVcwJAkSZIkSa33P4/rf5tUQ0yhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyPF1j5JGUrt",
        "colab_type": "text"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxJyhEQfGO-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "435e7bb5-7287-432f-e804-fc2bd17dcb99"
      },
      "source": [
        "# Apply the trained model to our test data (that is, the held-out node labels)\n",
        "# and measure how it performs\n",
        "results = model.evaluate(\n",
        "    initial_state, \n",
        "    y_test, \n",
        "    steps=1, \n",
        "    batch_size=NODE_COUNT, \n",
        "    verbose=0)\n",
        "\n",
        "# Get test metrics\n",
        "results_dict = dict(zip(model.metrics_names, results))\n",
        "for name in model.metrics_names:\n",
        "  if \"test\" not in name:\n",
        "    del results_dict[name]\n",
        "\n",
        "# Add train accuracy from the earlier training history\n",
        "results_dict[\"train_accuracy\"] = history.history['train_accuracy'][-1]\n",
        "\n",
        "results_keys = list(results_dict.keys())\n",
        "results_values = [results_dict[key] for key in results_keys]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [15, 6]\n",
        "\n",
        "# Display a bar chart\n",
        "y_pos = np.arange(len(results_dict))\n",
        "plt.barh(y_pos, results_values, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, results_keys)\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Evaluation')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAF1CAYAAADPzjZfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJKlpDr6SJEmSpKZ9Agc2bwINJQK0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjlBnqU30QiX",
        "colab_type": "text"
      },
      "source": [
        "## Exercise questions and next steps\n",
        "\n",
        "\n",
        "[Add your answers and discoveries to the answer document](https://docs.google.com/document/d/1QdAEOYnJ5AwFczNQZk5ZZo1Ng7_shn33GzQPdumnsvI/edit?usp=sharing)\n",
        "\n",
        "\n",
        "- What train and test accuracy did you get?\n",
        "\n",
        "- What modifications did you try to the network? How did they perform?\n",
        "\n",
        "- Are there other graphs you find it useful to produce?\n",
        "\n",
        "- What is the theoretical capabilities of this network? Do we need other methods of graph machine learning?\n",
        "\n",
        "- Find an example of a Graph Convolutional Network being used in industry, link to it here and provide a summary\n",
        "\n",
        "- How would you scale this network to a larger graph? What challenges might you encounter?\n",
        "\n",
        "- Imagine you’re applying this method to Twitter’s tweet-reply graph. It’s constantly changing. How could you apply this method (which currently is written for a static graph)?\n",
        "\n",
        "- How could you apply this to language (e.g. how could you treat language as a graph?)\n",
        "\n"
      ]
    }
  ]
}